{"meta":{"title":"机锋小摩托的分享","subtitle":"","description":"个人学习及工作经验分享","author":"机锋小摩托","url":"https://zhangjunjunah.github.io","root":"/"},"pages":[{"title":"关于","date":"2020-03-22T01:24:33.077Z","updated":"2020-03-21T10:26:01.350Z","comments":false,"path":"about/index.html","permalink":"https://zhangjunjunah.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2020-03-22T02:26:39.002Z","updated":"2020-03-22T02:26:39.002Z","comments":false,"path":"books/index.html","permalink":"https://zhangjunjunah.github.io/books/index.html","excerpt":"","text":"Effective Java Java 编程思想"},{"title":"分类","date":"2020-03-21T10:26:01.351Z","updated":"2020-03-21T10:26:01.351Z","comments":false,"path":"categories/index.html","permalink":"https://zhangjunjunah.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-03-22T02:29:57.368Z","updated":"2020-03-22T02:29:57.368Z","comments":false,"path":"tags/index.html","permalink":"https://zhangjunjunah.github.io/tags/index.html","excerpt":"","text":"ceph"}],"posts":[{"title":"zeppelin 0.8.0 适配FusionInsight_HD_6.5.1.7","slug":"zeppelin-0-8-0-适配FusionInsight-HD-6-5-1-7","date":"2020-12-03T16:00:00.000Z","updated":"2020-12-04T09:38:52.847Z","comments":true,"path":"2020/12/04/zeppelin-0-8-0-适配FusionInsight-HD-6-5-1-7/","link":"","permalink":"https://zhangjunjunah.github.io/2020/12/04/zeppelin-0-8-0-%E9%80%82%E9%85%8DFusionInsight-HD-6-5-1-7/","excerpt":"","text":"适配背景​ 最近客户的华为集群升级到6.5.1.7，我们安装的zeppelin又不好使了。。。 适配方法​ 适配方法参考一篇blog，在这里列出主要步骤 ​ 1.将/opt/hadoopclient/Spark2x/spark/jars路径下所有的jar包拷贝至/usr/zeppelin/interpreter/spark 1cp &#x2F;opt&#x2F;hadoopclient&#x2F;Spark2x&#x2F;spark&#x2F;jars&#x2F;*.jar &#x2F;usr&#x2F;zeppelin&#x2F;interpreter&#x2F;spark&#x2F; ​ 2.编辑zeppelin-env.sh文件，位置/usr/zeppelin/conf，加入以下内容 1234export JAVA_HOME&#x3D;&#x2F;opt&#x2F;hadoopclient&#x2F;JDK&#x2F;jdk-8u201export MASTER&#x3D;yarn-clientexport SPARK_HOME&#x3D;&#x2F;opt&#x2F;hadoopclient&#x2F;Spark2x&#x2F;sparkexport HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoopclient&#x2F;HDFS&#x2F;hadoop&#x2F;etc&#x2F;hadoop 步骤比较简单，测试提交到yarn集群，却提示(这个问题困扰了我几天。。。) 1234567891011java.lang.ClassCastException: org.apache.hadoop.conf.Configuration cannot be cast to org.apache.hadoop.yarn.conf.YarnConfiguration at org.apache.spark.deploy.yarn.ApplicationMaster.&lt;init&gt;(ApplicationMaster.scala:60) at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$main$1.apply$mcV$sp(ApplicationMaster.scala:679) at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:69) at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:68) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917) at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:68) at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:678) at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala) 下面给出我这边解决方法 适配问题解决网上提示这个报错的解决方法 由于环境中版本不一致导致，需要确保环境中spark版本和hadoop版本为匹配版本： 1.确认spark版本与hadoop版本一致。 2.如果是spark on yarn，同时确认spark配置文件中的spark.yarn.archive或spark.yarn.jar的spark版本。 ​ 我检查了下，zeppelin ${zeppelin_home}/interpreter/spark/ 和 ${zeppelin_home}/lib下的hadoop版本，发现${zeppelin_home}/lib下的hadoop还是2.×的版本(FusionInsight_HD_6.5.1.7 hadoop 3.1.1)。于是满怀期待的替换了，测试[裂开了]还是不好使，依然报同样的错。 ​ 找不到问题只好增加点日志，看看应用读取的spark_defaults.conf，发现了个奇怪的现象，spark.yarn.archive参数有重复。 12spark.yarn.archive value:hdfs://hacluster/user/spark2x/jars/V100R002C80SPC200/spark-archive-2x.zipspark.yarn.archive:hdfs://hacluster/user/spark2x/jars/6.5.1.7/spark-archive-2x.zip 但查看spark_defaults.conf也只发现下面的那一项，V100R002C80SPC200那个没找打在哪配置。。。没办法，修改下zeppelin的源码手动剔除这个配置项，重启问题解决😊。 问题原因：就是spark.yarn.archive配置了不明版本的spark-archive-2x.zip 参考文档Zeppelin连接Spark","categories":[],"tags":[{"name":"spark","slug":"spark","permalink":"https://zhangjunjunah.github.io/tags/spark/"}]},{"title":"在容器中提交yarn-client远端spark任务","slug":"在容器中提交yarn-client远端spark任务","date":"2020-11-09T01:44:17.000Z","updated":"2020-11-10T01:54:27.795Z","comments":true,"path":"2020/11/09/在容器中提交yarn-client远端spark任务/","link":"","permalink":"https://zhangjunjunah.github.io/2020/11/09/%E5%9C%A8%E5%AE%B9%E5%99%A8%E4%B8%AD%E6%8F%90%E4%BA%A4yarn-client%E8%BF%9C%E7%AB%AFspark%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"spark在容器中提交yarn-client任务方案一：容器端口nodePort映射方式方案说明​ 默认情况下， spark-submit 使用pod的 hostname 作为 spark.driver.host ，而 hostname 是pod的主机名，因此 spark executor 无法解析它，而且 spark.driver.port 也在pod（容器）的本地。此方案是通过端口映射方式连接yarn集群。 创建deployement和service的同时，通过nodePort方式暴露spark.driver.port和spark.blockManager.port两个端口 spark-submit提交命令，添加参数 –conf spark.driver.bindAddress=0.0.0.0 #固定值 –conf spark.driver.host=$HOST_IP #k8s节点ip,可以固定为k8s master节点ip（非宿主节点） –conf spark.driver.port=$SPARK_DRIVER_PORT #通过nodePort暴露的端口 –conf spark.driver.blockManager.port=$SPARK_DRIVER_PORT#通过nodePort暴露的端口 方案二：hostNetwork模式方案说明​ 将pod修改为hostNetwork模式，容器可以使用宿主机的网络协议栈，复用宿主机的IP，可以监听端口，Spark也可以回调。 方案三：路由转发方案说明​ 之所以在容器内无法提交spark yarn-client任务，是因为容器内网与hadoop集群不互通，只要想办法解决网络互通，问题自然而然就解决了。下面是我们测试环境的网络转发图。 参考blog在Kubernetes的pod中使用yarn-client调用远端spark集群Spark/k8s：如何用客户机模式在Kubernetes上运行Spark提交k8s骚操作 Pod服务通过IP对外访问&amp;k8s指定IP创建Pod","categories":[],"tags":[{"name":"K8S","slug":"K8S","permalink":"https://zhangjunjunah.github.io/tags/K8S/"}]},{"title":"一次k8s 6443 端口 was refused 解决步骤及思路","slug":"一次k8s-6443-端口-was-refused-解决步骤及思路","date":"2020-10-25T01:19:07.000Z","updated":"2020-10-25T05:00:08.554Z","comments":true,"path":"2020/10/25/一次k8s-6443-端口-was-refused-解决步骤及思路/","link":"","permalink":"https://zhangjunjunah.github.io/2020/10/25/%E4%B8%80%E6%AC%A1k8s-6443-%E7%AB%AF%E5%8F%A3-was-refused-%E8%A7%A3%E5%86%B3%E6%AD%A5%E9%AA%A4%E5%8F%8A%E6%80%9D%E8%B7%AF/","excerpt":"","text":"一、问题现象​ 周一上午同事向我反馈说咱们的测试k8s集群kubectl 命令连接不上提示The connection to the server ip:6443 was refused - did you specify the right host or port?，之前也遇到过这个问题。当时刚刚开始使用k8s，上面还没有pv，为了避免耽误开发卸载重装了(心想这次怎么也得先分析下问题，有点进步才行)，于是慢慢尝试，下面记录了我解决的全过程。 二、解决过程​ 查询了资料 1.检查节点状态1234567891011121314151617181920212223 systemctl status kubelet -l● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Mon 2020-10-12 14:16:53 CST; 26min ago Docs: https://kubernetes.io/docs/ Main PID: 2301 (kubelet) Tasks: 27 Memory: 134.8M CGroup: /system.slice/kubelet.service └─2301 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1Oct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.218811 2301 kubelet.go:2248] node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.318986 2301 kubelet.go:2248] node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.392439 2301 reflector.go:125] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://ip:6443/api/v1/pods?fieldSelector=spec.nodeName%3Ddeeplearn&amp;limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.419206 2301 kubelet.go:2248] node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.478573 2301 handler.go:321] HTTP InternalServerError serving /stats/summary: Internal Error: failed to get node info: node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.478791 2301 handler.go:321] HTTP InternalServerError serving /stats/summary: Internal Error: failed to get node info: node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.481661 2301 handler.go:321] HTTP InternalServerError serving /stats/summary: Internal Error: failed to get node info: node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.519410 2301 kubelet.go:2248] node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.592251 2301 reflector.go:125] k8s.io/client-go/informers/factory.go:133: Failed to list *v1beta1.RuntimeClass: Get https://ip:6443/apis/node.k8s.io/v1beta1/runtimeclasses?limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.619630 2301 kubelet.go:2248] node \"deeplearn\" not found 没发现什么有价值的信息，于是又查看了系统日志 123456789101112[root@deeplearn log]# tail -f messagesOct 12 10:37:49 deeplearn systemd-logind: New session 1080012 of user root.Oct 12 10:37:49 deeplearn systemd: Started Session 1080012 of user root.Oct 12 10:37:49 deeplearn systemd: Starting Session 1080012 of user root.Oct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.044418 2544 kuberuntime_manager.go:887] PodSandboxStatus of sandbox \"06785948be010cf48cbef180389fd38199517ce1bab59351de39a69ff12b4859\" for pod \"etcd-deeplearn_kube-system(1fdca295cf61791ef3ce86ddb8403ba7)\" error: rpc error: code = Unknown desc = Error response from daemon: open /home/docker/overlay2/2a32175b96d1bd9a6bfc3ff7c484a50c31074f900e7c0fe205c78f1fca8fa417/lower: too many open filesOct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.047370 2544 kuberuntime_manager.go:887] PodSandboxStatus of sandbox \"06785948be010cf48cbef180389fd38199517ce1bab59351de39a69ff12b4859\" for pod \"etcd-deeplearn_kube-system(1fdca295cf61791ef3ce86ddb8403ba7)\" error: rpc error: code = Unknown desc = Error response from daemon: open /home/docker/overlay2/2a32175b96d1bd9a6bfc3ff7c484a50c31074f900e7c0fe205c78f1fca8fa417/lower: too many open filesOct 12 10:37:50 deeplearn systemd-logind: Removed session 1080012.Oct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.148623 2544 reflector.go:125] object-\"kube-system\"/\"gpushare-device-plugin-token-sf97m\": Failed to list *v1.Secret: Get https://ip:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=metadata.name%3Dgpushare-device-plugin-token-sf97m&amp;limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.290671 2544 event.go:249] Unable to write event: 'Patch https://ip:6443/api/v1/namespaces/kube-system/events/etcd-deeplearn.163d0410148c7455: dial tcp ip:6443: connect: connection refused' (may retry after sleeping)Oct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.347797 2544 reflector.go:125] object-\"kube-system\"/\"kube-proxy-token-8fbbs\": Failed to list *v1.Secret: Get https://ip:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=metadata.name%3Dkube-proxy-token-8fbbs&amp;limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.547603 2544 reflector.go:125] k8s.io/kubernetes/pkg/kubelet/kubelet.go:444: Failed to list *v1.Service: Get https://ip:6443/api/v1/services?limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 10:37:50 deeplearn kubelet: W1012 10:37:50.747777 2544 status_manager.go:485] Failed to get status for pod \"kube-controller-manager-deeplearn_kube-system(ba945724332cd7f7efca6f11bdcf8307)\": Get https://ip:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-deeplearn: dial tcp ip:6443: connect: connection refused 发现了可疑日志too many open files too many open files是Linux系统中常见的错误，从字面意思上看就是说程序打开的文件数过多，不过这里的files不单是文件的意思，也包括打开的通讯链接(比如socket)，正在监听的端口等等，所以有时候也可以叫做句柄(handle)，这个错误通常也可以叫做句柄数超出系统限制。 是哪个进程占用了过多的句柄？ 发现docker 和k8s 占据前两名(8225是一个业务进程) 1234[root@deeplearn ~]# lsof -n|awk '&#123;print $2&#125;'|sort|uniq -c|sort -nr|more5901300 27593 20532 1081 17152 8225 2.清理系统句柄​ 尝试使用docker system df清理也提示error reading dir entries: open /run/docker/plugins: too many open files（使用ulimit -a查询open files参数已调至系统上限），没办法只有先停止后面的业务进程了。 ​ 杀掉业务系统，执行docker 清理命令，文件句柄数下降但k8s依然没有恢复正常。。。 123456docker system prune docker image prunedocker container prunedocker volume prune docker network prunedocker rmi $(docker images -q) 3.导入基础镜像​ 到这里问题线索又断了，没办法再上网查查看看有没有类似的问题现象吧。在一篇blog中发现了类似的日志《问题“The connection to the server….:6443 was refused - did you specify the right host or port?”的处理！》。 按照blog中的解决方法–重新导入基础镜像,问题解决！ 三、问题总结 ​ 基础镜像无故消失，导致问题产生。 ​ 系统文件句柄数过高,导致系统服务异常。","categories":[],"tags":[{"name":"K8S","slug":"K8S","permalink":"https://zhangjunjunah.github.io/tags/K8S/"}]},{"title":"Java操作docker步骤及问题总结","slug":"Java操作docker步骤及问题总结","date":"2020-07-18T05:33:06.000Z","updated":"2020-07-18T07:47:00.223Z","comments":true,"path":"2020/07/18/Java操作docker步骤及问题总结/","link":"","permalink":"https://zhangjunjunah.github.io/2020/07/18/Java%E6%93%8D%E4%BD%9Cdocker%E6%AD%A5%E9%AA%A4%E5%8F%8A%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","excerpt":"","text":"一、前言​ 这段时间一直在使用docker和kubernetes，最近产品有个需求在业务上迭代镜像，需要通过Java 远程操作docker。我想这种方案网上应该挺多的，很容易完成，但工作不是像我想的那样顺利。。。下面附上操作步骤及遇到的一些问题和我的解决思路。 二、操作步骤(参考网上)​ 此部分主要参考网上的一些blog 配置Docker两台docker主机(linux Centos 7)，其中一台使用GPU(不是我负责的，机器环境不太熟，这里留了一个坑) 1.修改docker.service，ExecStart=/usr/bin/dockerd 添加上 -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 12345678vi /lib/systemd/system/docker.service[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by docker#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 2.修改后保存文件，然后通知docker服务做出的修改 12systemctl daemon-reloadservice docker restart 3.测试使用docker命令远程方式访问 1docker -H tcp://ip:2375 images 配置产品使用docker-javadocker-java推荐使用maven项目引入相关的jar包依赖： 123456&lt;dependency&gt; &lt;groupId&gt;com.github.docker-java&lt;/groupId&gt; &lt;artifactId&gt;docker-java&lt;/artifactId&gt; &lt;!-- use latest version https://github.com/docker-java/docker-java/releases --&gt; &lt;version&gt;3.X.Y&lt;/version&gt;&lt;/dependency&gt; 三、问题解决方法上面的步骤配置不复杂，但实际操作后，还是出现了些问题，在这里记录一下。 1.在/lib/systemd/system/docker.service配置了-H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock，但是配置无效 解决方法: 进入/etc/systemd/system/docker.service.d目录，检查override.conf文件是否覆盖了[Service] ExecStart的配置 2.产品配置了docker-java maven依赖项目连接失败并提示报错: 123456789Exception in thread \"main\" java.lang.NoSuchMethodError: javax.ws.rs.core.MultivaluedMap.addAll(Ljava/lang/Object;[Ljava/lang/Object;)V at org.glassfish.jersey.client.ClientRequest.accept(ClientRequest.java:336) at org.glassfish.jersey.client.JerseyInvocation$Builder.accept(JerseyInvocation.java:240) at org.glassfish.jersey.client.JerseyInvocation$Builder.accept(JerseyInvocation.java:163) at com.github.dockerjava.jaxrs.CommitCmdExec.execute(CommitCmdExec.java:35) at com.github.dockerjava.jaxrs.CommitCmdExec.execute(CommitCmdExec.java:15) at com.github.dockerjava.jaxrs.AbstrSyncDockerCmdExec.exec(AbstrSyncDockerCmdExec.java:23) at com.github.dockerjava.core.command.AbstrDockerCmd.exec(AbstrDockerCmd.java:35) at com.github.dockerjava.core.command.CommitCmdImpl.exec(CommitCmdImpl.java:347) 解决方法: ​ 此问题的原因是jar包版本冲突，检查项目中是否使用了hadoop相关jar包，去除jersey-core和jersey-server相关依赖 12345678910111213141516171819202122232425262728293031323334&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;$&#123;cdh.hadoop.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;jersey-json&lt;/artifactId&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-server&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;cdh.hadoop.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-server&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 四、解决思路​ 第三部分是我操作过程中遇到的两个问题的解决方法，解决方法看上去也挺简单的，但解决的过程也比较的心酸(水平有待提高)，特意记录下。 ​ 1.GPU主机配置了Docker远程访问，但配置无效。。。 当时第一反应是看看有没有其他的配置方法，可能GPU主机配置了nvidia的相关信息，换种方法试试。修改/etc/docker/daemon.json，添加键值对 &quot;hosts&quot;: [&quot;0.0.0.0:2375&quot;,&quot;unix:///var/run/docker.sock&quot;]，但事与愿违，这种方式docker压根启动不了。在添加这段内容时，也发现了现象，即使我把deamon.json里的配置都删了，通过systemctl status docker还是能够看到dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime。 由于当时还有其他任务，就向当时安装这台GPU主机docker的同事请求协助，可能有啥特殊配置我不知道。过了两天，我这边紧急的事忙完了，回过来解决遗留的问题，自己的事要负责到底啊(下面重点来了)。 ​ 我自己还是坚持认为是有啥特殊配置导致这里不生效，二话不说我全局搜一下docker相关的配置文件不就行了，find . -name *docker*,依次检查了相关的配置没发现什么问题。上网搜了下docker有哪些配置文件，依然没什么收获。这里有点头大，于是我换了一种思路检查问题，我不是配置了两台主机了，那台CPU机器可以正常使用于是我分别用docker info和systemctl status docker命令查看两台主机有什么区别，终于在执行了systemctl status docker发现了些区别。 1234567891011121314151617181920212223242526272829 #GPU机器 systemctl status docker -l● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/docker.service.d └─override.conf Active: active (running) since Sat 2020-07-18 11:37:53 CST; 1h 39min ago Docs: https://docs.docker.com Main PID: 19626 (dockerd) Tasks: 91 Memory: 183.1M CGroup: /system.slice/docker.service ├─19626 /usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime ├─19865 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 20000 -container-ip 192.167.0.3 -container-port 20000 ├─21907 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9380 -container-ip 192.167.0.2 -container-port 9380 └─21925 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9360 -container-ip 192.167.0.2 -container-port 9360 #CPU机器 systemctl status docker -l● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2020-07-14 16:09:34 CST; 3 days ago Docs: https://docs.docker.com Main PID: 17952 (dockerd) Tasks: 92 Memory: 1.4G CGroup: /system.slice/docker.service ├─17952 /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock └─18512 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 1514 -container-ip 172.18.0.7 -container-port 10514 GPU机器多了一段Drop-In,查看override.conf,发现了问题所在，这里覆写了ExecStart相关配置，至此这个问题终于解决了。 12Drop-In: /etc/systemd/system/docker.service.d └─override.conf","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zhangjunjunah.github.io/tags/Docker/"},{"name":"Java","slug":"Java","permalink":"https://zhangjunjunah.github.io/tags/Java/"}]},{"title":"flannel组件挂了导致k8s网络通信失败","slug":"flannel组件挂了导致k8s网络通信失败","date":"2020-06-09T02:34:53.633Z","updated":"2020-06-09T02:34:53.633Z","comments":true,"path":"2020/06/09/flannel组件挂了导致k8s网络通信失败/","link":"","permalink":"https://zhangjunjunah.github.io/2020/06/09/flannel%E7%BB%84%E4%BB%B6%E6%8C%82%E4%BA%86%E5%AF%BC%E8%87%B4k8s%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%A4%B1%E8%B4%A5/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"docker 镜像瘦身(commit 方式制作的镜像)","slug":"docker 镜像瘦身(commit 方式制作的镜像)","date":"2020-04-22T16:00:00.000Z","updated":"2020-04-23T06:20:04.649Z","comments":true,"path":"2020/04/23/docker 镜像瘦身(commit 方式制作的镜像)/","link":"","permalink":"https://zhangjunjunah.github.io/2020/04/23/docker%20%E9%95%9C%E5%83%8F%E7%98%A6%E8%BA%AB(commit%20%E6%96%B9%E5%BC%8F%E5%88%B6%E4%BD%9C%E7%9A%84%E9%95%9C%E5%83%8F)/","excerpt":"","text":"背景​ 根据产品的运行作业的特点，我们需要制作一些业务镜像，将作业放到容器中执行。由于制作运行环境要不断调试，我们是通过大致(有省略)以下步骤制作的: 启动一个空容器(ubuntu) 安装python/pip 安装类库 将作业放到容器中测试;如果测试通过，docker commit 方式提交镜像 ​ 在这里要反思下，当时由于没有充分调研docker镜像正确的制作方式，导致后期迭代镜像越来越大(10G+)，以及想重新制作却没有了前面版本的制作轨迹。 ​ 近期，由于产品要在一个私有环境部署，镜像太大传输太慢，所以有了镜像瘦身的想法。 步骤 运行容器并进入，查看磁盘占用，删除不需要的数据 12345678#运行容器并进入容器样例docker run -d --name ubuntu ubuntu:15.10docker exec -i -t ubuntu /bin/bash#进入根目录cd / #查看各个目录体积du -h -d 1#删除不需要的数据(略) 在容器根目录打包容器数据(剔除/proc、/sys目录) 1234#通过tar方式打包tar --exclude=proc --exclude=sys --exclude=base_img.tar -cvf images.tar .#完成后退出容器exit 将容器内的tar包拷贝出来，再将镜像重新导入 1234#拷贝容器中的文件到宿主机中docker cp ubuntu:/images.tar .#将镜像重新导入cat images.tar|docker import - new-ubuntu 完成瘦身，比较瘦身前后的体积 1docker images |grep new-ubuntu 参考博文docker容器commit的镜像越来越大怎么办？酱紫试试","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zhangjunjunah.github.io/tags/Docker/"}]},{"title":"Log4J2.xml自用配置","slug":"Log4J2.xml自用配置","date":"2020-04-20T13:53:40.000Z","updated":"2020-04-21T03:07:34.360Z","comments":true,"path":"2020/04/20/Log4J2.xml自用配置/","link":"","permalink":"https://zhangjunjunah.github.io/2020/04/20/Log4J2.xml%E8%87%AA%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"","text":"前言​ log4j2的配置文件网上一搜很容易搜到，这里主要就是记录下自己开发中经常使用的一套配置，方便以后查找。 配置文件详情123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!--Configuration后面的status，这个用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，你会看到log4j2内部各种详细输出--&gt;&lt;!--monitorInterval：Log4j能够自动检测修改配置 文件和重新配置本身，设置间隔秒数--&gt;&lt;configuration status=\"error\" monitorInterval=\"30\"&gt; &lt;!--全局参数--&gt; &lt;Properties&gt; &lt;Property name=\"pattern\"&gt;[%style&#123;%d&#125;&#123;bright,green&#125;][%highlight&#123;%p&#125;][%style&#123;%t&#125;&#123;bright,blue&#125;][%style&#123;%C.%M:%L&#125;&#123;bright,yellow&#125;]: %msg%n%style&#123;%throwable&#125;&#123;red&#125;&lt;/Property&gt; &lt;Property name=\"logDir\"&gt;$&#123;env:IM_HOME&#125;/logs&lt;/Property&gt; &lt;Property name=\"disableAnsi\"&gt;false&lt;/Property&gt; &lt;/Properties&gt; &lt;Loggers&gt; &lt;Root level=\"INFO\"&gt; &lt;AppenderRef ref=\"console\"/&gt; &lt;AppenderRef ref=\"rolling_file\"/&gt; &lt;/Root&gt; &lt;/Loggers&gt; &lt;Appenders&gt; &lt;!-- 定义输出到控制台 --&gt; &lt;Console name=\"console\" target=\"SYSTEM_OUT\" follow=\"true\"&gt; &lt;!--控制台只输出level及以上级别的信息--&gt; &lt;ThresholdFilter level=\"DEBUG\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;PatternLayout &gt; &lt;Pattern&gt;$&#123;pattern&#125;&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;/Console&gt; &lt;!-- 同一来源的Appender可以定义多个RollingFile，定义按天存储日志 --&gt; &lt;RollingFile name=\"rolling_file\" fileName=\"$&#123;logDir&#125;/im-server.log\" filePattern=\"$&#123;logDir&#125;/im-server_%d&#123;yyyy-MM-dd&#125;.log\"&gt; &lt;ThresholdFilter level=\"INFO\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;$&#123;pattern&#125;&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy interval=\"1\"/&gt; &lt;/Policies&gt; &lt;!-- 日志保留策略，配置只保留七天 --&gt; &lt;DefaultRolloverStrategy&gt; &lt;Delete basePath=\"$&#123;logDir&#125;/\" maxDepth=\"1\"&gt; &lt;IfFileName glob=\"im-server_*.log\" /&gt; &lt;IfLastModified age=\"7d\" /&gt; &lt;/Delete&gt; &lt;/DefaultRolloverStrategy&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt;&lt;/configuration&gt; 说明 控制台日志高亮设置(IDE为IDEA),在启动主类VM options 中添加一行参数:-Dlog4j.skipJansi=false 在logDir属性配置中可以通过启动类的环境变量(Environment variables)设置IM_HOME路径","categories":[],"tags":[{"name":"Log4J2","slug":"Log4J2","permalink":"https://zhangjunjunah.github.io/tags/Log4J2/"}]},{"title":"Spring AOP结合自定义注解实例","slug":"spring AOP结合自定义注解实例","date":"2020-03-30T16:00:00.000Z","updated":"2020-04-21T02:57:39.846Z","comments":true,"path":"2020/03/31/spring AOP结合自定义注解实例/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/31/spring%20AOP%E7%BB%93%E5%90%88%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E5%AE%9E%E4%BE%8B/","excerpt":"","text":"使用背景​ 近期同事使用产品经常遇到页面点击卡死长时间未响应现象(本人比较擅长解决问题)，心里带着疑问使用jstack命令查看jvm堆栈，发现阻塞的线程都是表更新操作(update)。查看了下代码发现是由于最近大家申请容器资源频繁，申请容器逻辑在一个事务中，又因为资源告警创建时间变长，导致锁表。在这里简单说下申请容器的步骤。 完成资源申请合法校验，更新资源表信息 通过k8sApi创建需要的资源(pod、deployment、job、…) –在资源紧张时创建时间较长 创建失败删除对应的k8s资源 备注：资源表记录了容器平台资源总共多少资源（CPU、GPU、memory），已使用多少等信息 逻辑很简单，但创建容器频繁容易造成锁表，导致页面卡死。改造方案也很简单，更新资源表不走事务，如果创建逻辑失败，手动回滚，不就可以了。代理设计模式最适合这种改造场景，话不多说赶紧动手。 使用实例创建注解123456789101112/*** @Description: 申请容器(服务)资源注解* @Param:* @return:* @Author: zhangjj* @Date: 2020-02-25*/@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface ApplyContainerRes &#123;&#125; 编写切面在这里只放上了相关的代码（包名涉及公司暂时屏蔽了） 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @Description: 申请资源切面 * @ClassName: ApplyContainerResAspect * @Author: zhangjj * @Date: 2020-02-25 */@Aspect@Component@Slf4jpublic class ApplyContainerResAspect &#123; @Autowired private DataSourceTransactionManager dataSourceTransactionManager; @Pointcut(\"@annotation(xx.ApplyContainerRes)\") public void annotationApplyContainerResPointCut()&#123; &#125; @Before(\"annotationApplyContainerResPointCut()\") public void doBefore(JoinPoint joinPoint) &#123; DefaultTransactionDefinition def = new DefaultTransactionDefinition(); //新发起一个事务 def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW); TransactionStatus transaction = dataSourceTransactionManager.getTransaction(def); //伪代码 //申请资源(更新资源表信息) applyRes(joinPoint); dataSourceTransactionManager.commit(transaction); &#125; @AfterThrowing(\"annotationApplyContainerResPointCut()\") public void doAfter(JoinPoint joinPoint) &#123; DefaultTransactionDefinition def = new DefaultTransactionDefinition(); //新发起一个事务 def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW); TransactionStatus transactionStatus = dataSourceTransactionManager.getTransaction(def); //伪代码 //释放资源(更新资源表信息) releaseResource(joinPoint); dataSourceTransactionManager.commit(transactionStatus); &#125;&#125; 业务使用service层业务逻辑样例 12345678910111213141516/*** @Description: 启动服务* @Param: [environmentId,modelServices]* @return: OperContainerResult* @Author: zhangjj* @Date: 2019-08-07*/@Override@ApplyContainerRespublic OperContainerResult startServing(Long environmentId, ModelServices modelServices) &#123; OperContainerResult operContainerResult = new OperContainerResult(); //服务部署 deployModelService(modelServices); operContainerResult.setCode(OperContainerResult.CODE.SUCCESS); return operContainerResult;&#125; 改造完成，测试通过！很顺利嘛(手动怀疑)，可以让同事仿照上面样例改造其他业务逻辑了。 使用中的问题​ 同事的改造速度也很快，本地测试发现不对，资源表的数据怎么没更新啊！我这边心想咋回事，我这边使用都好使，怎么你这却不好使了。。。一起查看逻辑，发现他那的逻辑是在同一个类中调用。我这边意识到动态代理好像不能代理类中方法的直接调用，网上搜搜看看有没有解决方法,很快找到了解决办法。 未调整前代码 1234567891011121314151617181920/** * @param batchWorkId * @Description: 批量作业运行 * @Param: [batchWorkId] * @return: void * @Author: zhangjj * @Date: 2020-01-03 */@Overridepublic void runBatchWork(Long batchWorkId) &#123; //其他业务 dosomething(); //调用被代理的代理类 doRunBatchWork(batchWorkId);&#125;@ApplyContainerRespublic void doRunBatchWork(batchWorkId) &#123; //业务逻辑&#125; 网上的解决方案 既然 doRunBatchWork() 方法调用没有触发 AOP 逻辑的原因是因为我们以目标对象的身份(target object) 来调用的, 那么解决的关键自然就是以代理对象(proxied object)的身份来调用 doRunBatchWork() 方法。 备注:由于这里的调用方法没有声明在接口中，需要将jdk动态代理调整为cglib动态代理(这里先留个坑，以后可以总结下两者的区别) 调整后的代码 12345678910111213141516171819202122232425262728//在springboot启动类上添加下面一行注解@EnableAspectJAutoProxy(exposeProxy=true,proxyTargetClass=true)//业务伪代码@Autowiredprivate XXServiceImpl self;/** * @param batchWorkId * @Description: 批量作业运行 * @Param: [batchWorkId] * @return: void * @Author: zhangjj * @Date: 2020-01-03 */@Overridepublic void runBatchWork(Long batchWorkId) &#123; //其他业务 dosomething(); //调用被代理的代理类 self.doRunBatchWork(batchWorkId);&#125;@ApplyContainerRespublic void doRunBatchWork(batchWorkId) &#123; //业务逻辑&#125; 修改后，代理逻辑生效。不过，没过多久同事又反馈另外一个逻辑这么改造还是不好使，我查看了下发现方法声明是private(被代理的类必须要是public声明)。之前没意识到spring AOP使用中会遇到这些问题，在这里也简单总结下，方便查看回忆。 使用总结 springboot使用cglib代码，在启动类上添加以下注解@EnableAspectJAutoProxy(exposeProxy=true,proxyTargetClass=true) 被代理的方法尽量跨类调用 被代理的方法不能声明为私有方法","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://zhangjunjunah.github.io/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://zhangjunjunah.github.io/tags/Spring/"}]},{"title":"记录一次因内存原因导致k8s创建pod失败问题","slug":"记录一次因内存原因导致k8s创建pod失败问题","date":"2020-03-23T16:00:00.000Z","updated":"2020-04-21T02:57:39.847Z","comments":true,"path":"2020/03/24/记录一次因内存原因导致k8s创建pod失败问题/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/24/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E5%9B%A0%E5%86%85%E5%AD%98%E5%8E%9F%E5%9B%A0%E5%AF%BC%E8%87%B4k8s%E5%88%9B%E5%BB%BApod%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题背景​ 最近同事反馈使用k8s创建jupyter经常在某一台节点创建不起来(其他节点正常)，如果把这台节点的docker重启，当时可以恢复正常，但过段时间问题又会复现,下面附上kubectl describe部分日志。 12345Events: Type Reason Age From Message---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 1s (x13 over 18s) kubelet, centos7-141 Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \"jupyter-lab1584951113027\": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:303: getting the final child's pid from pipe caused \\\"EOF\\\"\": unknown 问题分析​ 带着疑问上网查询报错，有的说是docker版本、centos 内核与k8s版本不兼容导致？ ​ 依次查询了几台主机的docker版本(k8s是一起安装的所以直接排除)，都是19.03.5版本(centos版本也一致)。但仔细想想其他节点都没出现这个问题，可能问题原因不在这，暂时排除。 123456789101112131415161718192021222324252627Client: Docker Engine - Community Version: 19.03.5 API version: 1.40 Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:25:41 2019 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 19.03.5 API version: 1.40 (minimum version 1.12) Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:24:18 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.6 GitCommit: 894b81a4b802e4eb2a91d1ce216b8817763c29fb runc: Version: 1.0.0-rc8 GitCommit: 425e105d5a03fabd737a126ad93d62a9eeede87f docker-init: Version: 0.18.0 GitCommit: fec3683 ​ 问题困扰了一两天，后来看到一篇文章Kubernetes因限制内存配置引发的错误给我启发，会不会是内存原因导致的问题。使用free 命令对比了两台节点的内存。 1234567891011# 问题节点[root@centos7-141 log]# free -g total used free shared buff/cache availableMem: 22 1 0 1 20 12Swap: 0 0 0# 正常节点[root@centos7-142 ~]# free -g total used free shared buff/cache availableMem: 22 2 9 1 10 15Swap: 0 0 0 等等，发现了一些端倪，为啥总计22g，只使用1g,剩余内存为啥是0？还有buff/cache 是啥？带着疑问，查询了 buff/cache,在这记录下。 Linux服务器运行一段时间后，由于其内存管理机制，会将暂时不用的内存转为buff/cache，这样在程序使用到这一部分数据时，能够很快的取出，从而提高系统的运行效率，所以这也正是linux内存管理中非常出色的一点，所以乍一看内存剩余的非常少，但是在程序真正需要内存空间时，linux会将缓存让出给程序使用，这样达到对内存的最充分利用，所以真正剩余的内存是free+buff/cache 在这里按照先解决的原则，按照操作步骤释放了buff/cache。 123456echo 1 &gt; /proc/sys/vm/drop_caches#操作后的内存占用[root@centos7-141 log]# free -g total used free shared buff/cache availableMem: 22 1 7 1 13 12Swap: 0 0 重新创建pod，创建成功，问题解决！ 1234567891011Name: jupyter-lab1584951113027Namespace: kubeflowPriority: 0Node: centos7-141/192.168.128.141Start Time: Tue, 24 Mar 2020 14:00:46 +0800Labels: app=jupyterhub component=singleuser-server heritage=jupyterhubAnnotations: hub.jupyter.org/username: lab1584951113027Status: RunningIP: 10.244.3.3 后记​ 尽管问题解决了，但还是有两个疑问，在这里先记录下来 buff/cache为啥没有在k8s需要内存时及时让出？ 如果buff/cache没有及时让出缓存，为啥k8s events 不提示内存超出限制报错？ 参考 Kubernetes因限制内存配置引发的错误 Linux释放内存空间","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zhangjunjunah.github.io/tags/Docker/"},{"name":"K8S","slug":"K8S","permalink":"https://zhangjunjunah.github.io/tags/K8S/"}]},{"title":"Ceph 配置RGW高可用","slug":"ceph 配置 RGW高可用","date":"2020-03-20T16:00:00.000Z","updated":"2020-04-21T02:57:39.844Z","comments":true,"path":"2020/03/21/ceph 配置 RGW高可用/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/21/ceph%20%E9%85%8D%E7%BD%AE%20RGW%E9%AB%98%E5%8F%AF%E7%94%A8/","excerpt":"","text":"ceph节点规划 节点ip 节点主机名 节点ceph组件 其他相关组件 192.168.153.51 centos 7-ceph-1（ceph 主节点） mds1、mon1、mg1、rgw1、osd1 keepalived 192.168.153.52 centos 7-ceph-2 mon2、mg2、rgw2、osd2 keepalived 192.168.153.53 centos 7-ceph-3 mon3、mg3、osd3 整体架构 新增RGW服务 1234567891011#在centos 7-ceph-1节点使用cephuser用户cd my-clusterceph-deploy rgw create 128# 访问测试curl -I http://centos 7-ceph-1:7480/HTTP/1.1 200 OKx-amz-request-id: tx000000000000000000001-005d0b0e2a-1018-defaultContent-Type: application/xmlContent-Length: 0Date: Thu, 20 Jun 2019 04:40:11 GMT 安装相关软件安装依赖1234#在centos 7-ceph-1、centos 7-ceph-2节点使用root用户执行yum -y install openssl-devel --skip-brokenyum install -y libnl3-devel libnfnetlink-develyum -y install gcc 安装keepalived 12345678910111213141516171819#在centos 7-ceph-1、centos 7-ceph-2节点使用root用户执行cd /usr/local/src#官网下载keepalived的最新版本，解压并安装wget http://www.keepalived.org/software/keepalived-2.0.7.tar.gztar xvf keepalived-2.0.7.tar.gzcd keepalived-2.0.7./configure --prefix=/usr/local/keepalivedmake &amp;&amp; make install#初始化及启动#keepalived启动脚本变量引用文件，默认文件路径是/etc/sysconfig/，也可以不做软链接，直接修改启动脚本中文件路径即可（安装目录下）cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/keepalived # 将keepalived主程序加入到环境变量（安装目录下）cp /usr/local/keepalived/sbin/keepalived /usr/sbin/keepalived# keepalived启动脚本（源码目录下），放到/etc/init.d/目录下就可以使用service命令便捷调用cp /usr/local/src/keepalived-2.0.7/keepalived/etc/init.d/keepalived /etc/init.d/keepalived # 将配置文件放到默认路径下mkdir /etc/keepalivedcp /usr/local/keepalived/etc/keepalived/keepalived.conf etc/keepalived/keepalived.conf keepalived.conf配置12345678910111213141516171819202122232425262728293031#centos 7-ceph-1节点cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bakcat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123;&#125;vrrp_script chk_rgw &#123; script \"/usr/local/keepalived/sbin/check_rgw.sh\" # 该脚本检测rgw的运行状态，并在rgw进程挂了之后尝试重新启动rgw，如果启动失败则停止keepalived，准备让其它机器接管。 interval 2 # 每2s检测一次 weight 2 # 检测失败（脚本返回非0）则优先级2&#125;vrrp_instance VI_1 &#123; state MASTER # 指定keepalived的角色，MASTER表示此主机是主服务器，BACKUP表示此主机是备用服务器 interface ens32 # 指定HA监测网络的接口 根据你实际的网卡名来 keyong ip addr 查询 virtual_router_id 51 # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。即同一vrrp_instance下，MASTER和BACKUP必须是一致的 priority 100 # 定义优先级，数字越大，优先级越高，在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级 advert_int 1 # 设定MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS # 设置验证类型，主要有PASS和AH两种 auth_pass 1111 # 设置验证密码，在同一个vrrp_instance下，MASTER与BACKUP必须使用相同的密码才能正常通信 &#125; virtual_ipaddress &#123; 192.168.153.16 # 设置虚拟IP地址(与节点ip同网段) &#125; track_script &#123; chk_rgw # 引用VRRP脚本，即在 vrrp_script 部分指定的名字。定期运行它们来改变优先级，并最终引发主备切换。 &#125;&#125; 12345678910111213141516171819202122232425262728293031#centos 7-ceph-2节点cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bakcat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123;&#125;vrrp_script chk_rgw &#123; script \"/usr/local/keepalived/sbin/check_rgw.sh\" # 该脚本检测rgw的运行状态，并在rgw进程挂了之后尝试重新启动rgw，如果启动失败则停止keepalived，准备让其它机器接管。 interval 2 # 每2s检测一次 weight 2 # 检测失败（脚本返回非0）则优先级2&#125;vrrp_instance VI_1 &#123; state BACKUP # 指定keepalived的角色，MASTER表示此主机是主服务器，BACKUP表示此主机是备用服务器 interface ens32 # 指定HA监测网络的接口 根据你实际的网卡名来 keyong ip addr 查询 virtual_router_id 51 # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。即同一vrrp_instance下，MASTER和BACKUP必须是一致的 priority 100 # 定义优先级，数字越大，优先级越高，在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级 advert_int 1 # 设定MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS # 设置验证类型，主要有PASS和AH两种 auth_pass 1111 # 设置验证密码，在同一个vrrp_instance下，MASTER与BACKUP必须使用相同的密码才能正常通信 &#125; virtual_ipaddress &#123; 192.168.153.16 # 设置虚拟IP地址(与节点ip同网段) &#125; track_script &#123; chk_rgw # 引用VRRP脚本，即在 vrrp_script 部分指定的名字。定期运行它们来改变优先级，并最终引发主备切换。 &#125;&#125; centos 7-ceph-1/2 中，/usr/local/keepalived/sbin/check_rgw.sh脚本内容如下 1234567891011#!/bin/bashif [ \"$(ps -ef | grep \"radosgw\"| grep -v grep )\" == \"\" ];then systemctl start ceph-radosgw.target sleep 3 if [ \"$(ps -ef | grep \"radosgw\"| grep -v grep )\" == \"\" ];then systemctl stop keepalived fifi#添加check_rgw.sh脚本执行权限chmod +x /usr/local/keepalived/sbin/check_rgw.sh 到这里对keepalived的配置已经完成，然后分别启动centos 7-ceph-1/2点上的keepalived 1systemctl start keepalived 分别在centos 7-ceph-1和centos 7-ceph-2节点上执行ip a命令，查看虚IP信息： 1234567891011121314151617181920212223242526272829#centos 7-ceph-11: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:67:df:45 brd ff:ff:ff:ff:ff:ff inet 192.168.153.51/24 brd 192.168.129.255 scope global noprefixroute ens32 valid_lft forever preferred_lft forever inet 192.168.153.16/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::388c:a9c2:c50b:dd48/64 scope link noprefixroute valid_lft forever preferred_lft forever#centos 7-ceph-21: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:4c:7f:2a brd ff:ff:ff:ff:ff:ff inet 192.168.153.52/24 brd 192.168.129.255 scope global noprefixroute ens32 valid_lft forever preferred_lft forever inet6 fe80::58cc:e799:bc61:7fd5/64 scope link noprefixroute valid_lft forever preferred_lft forever 调整S3配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[cephuser@centos 7-ceph-1 keepalived-2.0.7]$ s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key [AY96HNDL9H2QV2SRPNDJ]: Secret Key [Q8OhUOJdRJg8KzAUwZ22BddL7QbC6g8hHCGwq1jx]: Default Region [US]: Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.S3 Endpoint [192.168.153.51:7480]: 192.168.153.16:7480Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [192.168.153.51:7480]: 192.168.153.16:7480Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: Path to GPG program [/bin/gpg]: When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [No]: On some networks all internet access must go through a HTTP proxy.Try setting it here if you can't connect to S3 directlyHTTP Proxy server name: New settings: Access Key: AY96HNDL9H2QV2SRPNDJ Secret Key: Q8OhUOJdRJg8KzAUwZ22BddL7QbC6g8hHCGwq1jx Default Region: US S3 Endpoint: 192.168.153.16:7480 DNS-style bucket+hostname:port template for accessing a bucket: 192.168.153.16:7480 Encryption password: Path to GPG program: /bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0Test access with supplied credentials? [Y/n] yPlease wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] yConfiguration saved to '/home/cephuser/.s3cfg'[cephuser@centos 7-ceph-1 keepalived-2.0.7]$ s3cmd ls2020-03-11 07:16 s3://MyBucket_12020-03-11 07:45 s3://algorithm-bucket2020-03-11 07:44 s3://demo-bucket 参考引用keepalived配置RGW高可用","categories":[],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://zhangjunjunah.github.io/tags/Ceph/"},{"name":"Linux","slug":"Linux","permalink":"https://zhangjunjunah.github.io/tags/Linux/"}]},{"title":"Ceph和s3的安装","slug":"ceph和s3的安装","date":"2020-03-20T16:00:00.000Z","updated":"2020-04-21T02:57:39.845Z","comments":true,"path":"2020/03/21/ceph和s3的安装/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/21/ceph%E5%92%8Cs3%E7%9A%84%E5%AE%89%E8%A3%85/","excerpt":"","text":"部署说明 安装的ceph版本（14.2.4）、s3 版本(2.0.2) ceph使用ceph-deploy部署高可用集群至少需要3台节点(ceph 监视器需要3个以上节点) ceph推荐使用裸盘安装osd(文件夹也可以部署，但不推荐) ceph部署节点规划 节点ip 节点主机名 节点ceph组件 192.168.153.51 centos 7-ceph-1（ceph 主节点） mds1、mon1、mg1、rgw、osd1 192.168.153.52 centos 7-ceph-2 mon2、mg2、osd2、 192.168.153.53 centos 7-ceph-3 mon3、mg3、osd3 创建部署用户注意，在外网环境，禁止使用 ceph 服务名 作为用户，防止暴力破解。 因为 ceph-deploy 部署工具需要以登录用户（且包含sudo权限）来安装软件和做配置，因此通常在主节点创建 ceph 用户。 12345678910111213141516171819# 在每一个节点执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）useradd cephuserecho 'cephuser' | passwd --stdin cephuserecho \"cephuser ALL = (root) NOPASSWD:ALL\" &gt; /etc/sudoers.d/cephuserchmod 0440 /etc/sudoers.d/cephuser# 配置sshd可以使用password登录sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_configsystemctl reload sshd# 配置sudo不需要ttysed -i 's/Default requiretty/#Default requiretty/' /etc/sudoers#每个节点添加hosts192.168.153.51 centos 7-ceph-1192.168.153.52 centos 7-ceph-2192.168.153.53 centos 7-ceph-3# 设置免密登录（使用cephuser用户）ssh-keygen -t rsa # 在centos 7-ceph-1使用cephuser用户操作ssh-copy-id -i /home/cephuser/.ssh/id_rsa.pub cephuser@centos 7-ceph-2ssh-copy-id -i /home/cephuser/.ssh/id_rsa.pub cephuser@centos 7-ceph-3 创建虚拟卷通过虚拟机管理界面分别为每个节点 挂载新磁盘 12345# 每台节点操作(root)fdisk -l #查看挂载的磁盘名，#格式化磁盘parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%mkfs.xfs /dev/sdb -f 关闭 SetLinux（建议） 12345# 建议执行，注意需要重启机器setenforce 0 sed -i \"s/SELINUX=enforcing/SELINUX=disabled /g\" /etc/selinux/config # 重启机器，检查，如果SELinux status参数为disabled即为关闭状态/usr/sbin/sestatus -v 关闭防火墙 123# 可选，如果不关闭需要开启对应端口，本例简单的关闭处理systemctl disable firewalld.service systemctl stop firewalld.service 配置ntp服务器 12345678910111213141516# 在三台节点root执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）yum -y install ntp# 将centos 7-ceph-1配置成ntp服务端[root@centos7-ceph-1 ~]# cat /etc/ntp.confserver 127.127.1.0fudge 127.127.1.0 stratum 10restrict 192.168.153.1 mask 255.255.255.0 nomodify notrap#启动ntpsystemctl start ntpd &amp;&amp; systemctl enable ntpd# centos 7-ceph-2、centos 7-ceph-3配置ntp客户端cat /etc/ntp.conf server 192.168.153.51#执行手动同步ntpdate 192.168.153.51systemctl start ntpd &amp;&amp; systemctl enable ntpd 开始部署安装 ceph 依赖 12345# ceph-deploy 是ceph 的部署工具，在管理节点(centos 7-ceph-1执行)yum install -y ceph-deploy ceph-deploy --version // 2.0.1# 安装 ceph 和 radosgw ,在每台节点执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）yum install -y ceph ceph-radosgw 搭建 ceph 服务 12345678910111213141516171819202122232425262728293031323334# 注意以下所有使用 ceph 用户执行，不要使用 sudo ，不要使用rootsu - cephusermkdir my-clustercd my-cluster# 创建集群，用 new 命令，并指定几个主机安装初始ceph Monitor 服务# 在centos 7-ceph-1上操作ceph-deploy new centos 7-ceph-1# 此时会在当前目录生成 ceph 配置文件、日志和密钥文件# 注意实际生效的配置文件将在 /etc/ceph/ceph.conf （当前还未生生成），最佳实践为修改 $HOME 目录下的文件配置，通过 deploy 工具分发到各节点lsceph.conf ceph-deploy-ceph.log ceph.mon.keyring# 需要修改一下配置文件echo \"public network = 192.168.153.0/24\" &gt;&gt; ceph.confecho \"cluster network = 192.168.153.0/24\" &gt;&gt; ceph.confecho \"osd pool default size = 3\" &gt;&gt; ceph.confecho \"osd pool default min size = 2\" &gt;&gt; ceph.confecho \"mon_max_pg_per_osd = 650\" &gt;&gt; ceph.confecho \"[osd]\" &gt;&gt; ceph.confecho \"mon_osd_backfillfull_ratio=0.7\" &gt;&gt; ceph.confecho \"mon_osd_full_ratio=0.8\" &gt;&gt; ceph.confecho \"mon_osd_nearfull_ratio=0.6\" &gt;&gt; ceph.confecho \"osd_failsafe_full_ratio=0.8\" &gt;&gt; ceph.conf# 部署monitor和生成keysceph-deploy mon create-initialls -l *.keyring# 复制文件到node节点ceph-deploy --overwrite-conf admin centos 7-ceph-1 centos 7-ceph-2 centos 7-ceph-3# 部署manager （luminous+）12及以后的版本需要部署ceph-deploy mgr create centos 7-ceph-1 创建 OSD（对象存储后台进程） 1234567891011#在centos 7-ceph-1用cephuser用户执行ceph-deploy osd create --data /dev/sdb centos 7-ceph-1 ceph-deploy osd create --data /dev/sdb centos 7-ceph-2 ceph-deploy osd create --data /dev/sdb centos 7-ceph-3 # 查看 osd tree和ceph集群状态sudo ceph osd treesudo ceph -ssudo ceph health detail //HEALTH_OK# 至此 基本ceph安装完毕 高可用配置(mon、mgr) 123456789# 添加monitorceph-deploy mon add centos 7-ceph-2ceph-deploy mon add centos 7-ceph-3# 添加managerceph-deploy mgr create centos 7-ceph-2 centos 7-ceph-3# 卸载monitorceph-deploy mon destroy centos 7-ceph-2 部署RGWCeph部署RGW以兼容S3/Swift接口，即可以使用S3或Swift的命令行工具或SDK来使用ceph。 部署RGW网关 12345678910111213141516171819202122232425# 启动 RGW，注意开放的网关端口，默认7480ceph-deploy rgw create centos 7-ceph-1 // The Ceph Object Gateway (RGW) is now running on host centos 7-ceph-1 and default port 7480# 访问测试curl -I http://centos 7-ceph-1:7480/HTTP/1.1 200 OKx-amz-request-id: tx000000000000000000001-005d0b0e2a-1018-defaultContent-Type: application/xmlContent-Length: 0Date: Thu, 20 Jun 2019 04:40:11 GMT# 页面访问测试 # 至此 Ceph 端对象访问接口已经部署完成，可使用s3 工具访问 # 附其他操作 # 附1：更改开放端口，需重启RGW# 修改配置 /etc/ceph/ceph.conf，使用 rgw 监听在 80 端口 [client.rgw.lab1]rgw_frontends = \"civetweb port=80\" # 附2：重启 RGWsystemctl restart ceph-radosgw@rgw.k8s113 对象存储服务与测试（S3/Swift）离线安装 s3cmd 12345rpm -ivh python-dateutil-1.5-7.el7.noarch.rpm python-magic-5.11-35.el7.noarch.rpm s3cmd-2.0.2-1.el7.noarch.rpms3cmd --versions3cmd version 2.0.2 对象存储测试（使用s3cmd测试） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141# 测试# 创建S3所需要的poolsudo ceph osd pool create .rgw 128 128sudo ceph osd pool create .rgw.root 128 128sudo ceph osd pool create .rgw.control 128 128sudo ceph osd pool create .rgw.gc 128 128sudo ceph osd pool create .rgw.buckets 128 128sudo ceph osd pool create .rgw.buckets.index 128 128sudo ceph osd pool create .rgw.buckets.extra 128 128sudo ceph osd pool create .log 128 128sudo ceph osd pool create .intent-log 128 128sudo ceph osd pool create .usage 128 128sudo ceph osd pool create .users 128 128sudo ceph osd pool create .users.email 128 128sudo ceph osd pool create .users.swift 128 128sudo ceph osd pool create .users.uid 128 128# 查看rados lspools# 访问测试curl -I http://192.168.153.51:7480/# 创建S3用户# 注意：保存命令返回的 user access_key secret_keysudo radosgw-admin user create --uid=s3_user --display-name=s3_user --email=s3@s3.com\"access_key\": \"FZ95GV7T0WU8OVOL4YH9\",\"secret_key\": \"WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPK\"# 创建admin用户radosgw-admin user create --uid=admin --display-name=admin\"access_key\": \"B3M00AHVE9FEXCQ8D0L9\",\"secret_key\": \"JVJG1O4LDGam35EBghXGsgrdOwhqGzVKPtR6ntw4\"# 允许admin读写所有users信息radosgw-admin caps add --uid=admin --caps=\"users=*\"# 允许admin读写所有的usage信息radosgw-admin caps add --uid=admin --caps=\"usage=read,write\"# 配置s3cmd（当前在 ceph目录下，附完整过程，输入之前创建的 s3_user_tydic 用户密钥） s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key: FZ95GV7T0WU8OVOL4YH9Secret Key: WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPKDefault Region [US]:Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.S3 Endpoint [s3.amazonaws.com]: 192.168.153.51:7480 //注意：此处必须使用 ip:port 形式，使用域名方式将会导致从s3cmd创建的桶必须是以大写字母开头，此命名与亚马逊官方桶命名要求相悖，建议修改为ip:port 方式。Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 192.168.153.51:7480 //ip:port 形式Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: //空Path to GPG program [/bin/gpg]: //空When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [Yes]: noOn some networks all internet access must go through a HTTP proxy.Try setting it here if you can't connect to S3 directlyHTTP Proxy server name: //空Test access with supplied credentials? [Y/n] yPlease wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] yConfiguration saved to '/home/ceph/.s3cfg'# 注意 s3 配置文件生成位置修该生成的配置文件，后续可做自定义修改# 重点配置项示例，其他项均为默认项[default]access_key = FZ95GV7T0WU8OVOL4YH9secret_key = WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPKhost_base = 192.168.153.51:7480host_bucket = 192.168.153.51:7480use_https = False# 创建Bucket（注意命令行端bucket首字母必须大写）s3cmd mb s3://MyBucket_1s3cmd ls# 上传Objectecho 'hello ceph block storage s3' &gt; hello.txts3cmd put hello.txt s3://MyBucket_1# 查看Objects3cmd ls s3://MyBucket_1# 下载Objectcd /tmps3cmd get s3://mybucket/hello.txtcat hello.txt# 删除bucket下所有对象s3cmd del -rf s3://MyBucket_1 s3cmd ls -r s3://MyBucket_1# 删除Buckets3cmd mb s3://MyBucket_1s3cmd rb s3://MyBucket_1# 其他操作# 删除S3用户radosgw-admin user rm --uid=s3_user_tydicradosgw-admin user rm --uid=admin#调整pool副本数ceph osd pool set default.rgw.buckets.data size 3ceph osd pool set default.rgw.buckets.data min_size 2# 删除poolceph osd pool delete .rgw .rgw --yes-i-really-really-mean-itceph osd pool delete .rgw.root .rgw.root --yes-i-really-really-mean-itceph osd pool delete .rgw.control .rgw.control --yes-i-really-really-mean-itceph osd pool delete .rgw.gc .rgw.gc --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets .rgw.buckets --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.index .rgw.buckets.index --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.extra .rgw.buckets.extra --yes-i-really-really-mean-itceph osd pool delete .log .log --yes-i-really-really-mean-itceph osd pool delete .intent-log .intent-log --yes-i-really-really-mean-itceph osd pool delete .usage .usage --yes-i-really-really-mean-itceph osd pool delete .users .users --yes-i-really-really-mean-itceph osd pool delete .users.email .users.email --yes-i-really-really-mean-itceph osd pool delete .users.swift .users.swift --yes-i-really-really-mean-itceph osd pool delete .users.uid .users.uid --yes-i-really-really-mean-it Ceph DashboardCeph Dashboard介绍Ceph 的监控可视化界面方案很多—-grafana、Kraken。但是从Luminous开始，Ceph 提供了原生的Dashboard功能，通过Dashboard可以获取Ceph集群的各种基本状态信息。 配置Ceph Dashboard安装和配置12345678910111213141516171819202122# 1、在每个mgr节点安装 yum install ceph-mgr-dashboard # 2、开启mgr功能 ceph mgr module enable dashboard # 3、生成并安装自签名的证书 ceph dashboard create-self-signed-cert # 4、创建一个dashboard登录用户名密码 ceph dashboard ac-user-create super 123456 administrator # 5、查看服务访问方式 ceph mgr services&#123; \"dashboard\": \"https://centos 7-ceph-1:8443/\"&#125;#安装完成#可选#修改ipsudo ceph config set mgr mgr/dashboard/server_addr 192.168.153.51#修改端口sudo ceph config set mgr mgr/dashboard/server_port 8443#禁用httpssudo ceph config set mgr mgr/dashboard/ssl false 安装问题汇总1.格式化磁盘时提示忙 1mkfs.xfs: cannot open /dev/sdb: Device or resource busy 解决方法 123456789lsblk #显示部分磁盘正常，部分下面有-ceph-**等标识，用ilo多次格式化磁盘作raid0均无效果dmsetup ls #查看谁在占用，找到ceph-**字样（ceph-**为lsblk显示的块设备具体信息）#使用dmsetup 删除字样dmsetup remove ceph-**lsblk #查看设备信息，可以看到ceph-**等标识等标识消失ceph-deploy disk zap centos7-ceph-3 /dev/sdb #格式化磁盘sudo parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%sudo mkfs.xfs /dev/sdb -f 2.ceph-deploy new centos 7-ceph-1 时，提示 ImportError: No module named pkg_resources 1234567[cephuser@centos 7-ceph-1 my-cluster]$ ceph-deploy new centos 7-ceph-1Traceback (most recent call last): File \"/bin/ceph-deploy\", line 18, in &lt;module&gt; from ceph_deploy.cli import main File \"/usr/lib/python2.7/site-packages/ceph_deploy/cli.py\", line 1, in &lt;module&gt; import pkg_resourcesImportError: No module named pkg_resources 解决方法 12345#安装pipsudo yum -y install epel-releasesudo yum -y install python-pip#安装distribute类库pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com distrbute 3.如果安装出错如何清理 12345# 卸载Ceph安装包ceph-deploy purge &lt;hostname&gt;# 清理配置ceph-deploy purgedata &lt;hostname&gt;ceph-deploy forgetkeys 安装ceph dashboard 提示 No module 1Module 'dashboard' has failed dependency: No module named urllib3.exceptions 解决方法 12#pip安装 urllib3pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com urllib3","categories":[],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://zhangjunjunah.github.io/tags/Ceph/"},{"name":"Linux","slug":"Linux","permalink":"https://zhangjunjunah.github.io/tags/Linux/"}]}]}