{"meta":{"title":"机锋小摩托的分享","subtitle":"","description":"个人学习及工作经验分享","author":"机锋小摩托","url":"https://zhangjunjunah.github.io","root":"/"},"pages":[{"title":"关于","date":"2020-03-22T01:24:33.077Z","updated":"2020-03-21T10:26:01.350Z","comments":false,"path":"about/index.html","permalink":"https://zhangjunjunah.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2020-03-22T02:26:39.002Z","updated":"2020-03-22T02:26:39.002Z","comments":false,"path":"books/index.html","permalink":"https://zhangjunjunah.github.io/books/index.html","excerpt":"","text":"Effective Java Java 编程思想"},{"title":"分类","date":"2020-03-21T10:26:01.351Z","updated":"2020-03-21T10:26:01.351Z","comments":false,"path":"categories/index.html","permalink":"https://zhangjunjunah.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-03-22T02:29:57.368Z","updated":"2020-03-22T02:29:57.368Z","comments":false,"path":"tags/index.html","permalink":"https://zhangjunjunah.github.io/tags/index.html","excerpt":"","text":"ceph"}],"posts":[{"title":"docker 镜像瘦身(commit 方式制作的镜像)","slug":"docker 镜像瘦身(commit 方式制作的镜像)","date":"2020-04-22T16:00:00.000Z","updated":"2020-04-23T03:05:37.104Z","comments":true,"path":"2020/04/23/docker 镜像瘦身(commit 方式制作的镜像)/","link":"","permalink":"https://zhangjunjunah.github.io/2020/04/23/docker%20%E9%95%9C%E5%83%8F%E7%98%A6%E8%BA%AB(commit%20%E6%96%B9%E5%BC%8F%E5%88%B6%E4%BD%9C%E7%9A%84%E9%95%9C%E5%83%8F)/","excerpt":"","text":"背景​ 根据产品的运行作业的特点，我们需要制作一些业务镜像，将作业放到容器中执行。由于制作运行环境要不断调试，我们是通过大致(有省略)以下步骤制作的: 启动一个空容器(ubuntu) 安装python/pip 安装类库 将作业放到容器中测试;如果测试通过，docker commit 方式提交镜像 ​ 在这里要反思下，当时由于没有充分调研docker镜像正确的制作方式，导致后期迭代镜像越来越大(10G+)，以及想重新制作却没有了前面版本的制作轨迹。 ​ 近期，由于产品要在一个私有环境部署，镜像太大传输太慢，所以有了镜像瘦身的想法。 步骤 运行容器并进入，查看磁盘占用，删除不需要的数据 12345678#运行容器并进入容器样例docker run -d --name ubuntu ubuntu:15.10docker exec -i -t ubuntu /bin/bash#进入根目录cd / #查看各个目录体积du -h -d 1#删除不需要的数据(略) 在容器根目录打包容器数据(剔除/proc、/sys目录) 1234#通过tar方式打包tar --exclude=proc --exclude=sys --exclude=base_img.tar -cvf images.tar .#完成后退出容器exit 将容器内的tar包拷贝出来，再将镜像重新导入 1234#拷贝容器中的文件到宿主机中docker cp ubuntu:/images.tar .#将镜像重新导入cat images.tar|docker import - new-ubuntu 完成瘦身，比较瘦身前后的体积 1docker images |grep new-ubuntu 参考博文docker容器commit的镜像越来越大怎么办？酱紫试试","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://zhangjunjunah.github.io/tags/docker/"}]},{"title":"Log4J2.xml自用配置","slug":"Log4J2.xml自用配置","date":"2020-04-20T13:53:40.000Z","updated":"2020-04-21T03:07:34.360Z","comments":true,"path":"2020/04/20/Log4J2.xml自用配置/","link":"","permalink":"https://zhangjunjunah.github.io/2020/04/20/Log4J2.xml%E8%87%AA%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"","text":"前言​ log4j2的配置文件网上一搜很容易搜到，这里主要就是记录下自己开发中经常使用的一套配置，方便以后查找。 配置文件详情123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!--Configuration后面的status，这个用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，你会看到log4j2内部各种详细输出--&gt;&lt;!--monitorInterval：Log4j能够自动检测修改配置 文件和重新配置本身，设置间隔秒数--&gt;&lt;configuration status=\"error\" monitorInterval=\"30\"&gt; &lt;!--全局参数--&gt; &lt;Properties&gt; &lt;Property name=\"pattern\"&gt;[%style&#123;%d&#125;&#123;bright,green&#125;][%highlight&#123;%p&#125;][%style&#123;%t&#125;&#123;bright,blue&#125;][%style&#123;%C.%M:%L&#125;&#123;bright,yellow&#125;]: %msg%n%style&#123;%throwable&#125;&#123;red&#125;&lt;/Property&gt; &lt;Property name=\"logDir\"&gt;$&#123;env:IM_HOME&#125;/logs&lt;/Property&gt; &lt;Property name=\"disableAnsi\"&gt;false&lt;/Property&gt; &lt;/Properties&gt; &lt;Loggers&gt; &lt;Root level=\"INFO\"&gt; &lt;AppenderRef ref=\"console\"/&gt; &lt;AppenderRef ref=\"rolling_file\"/&gt; &lt;/Root&gt; &lt;/Loggers&gt; &lt;Appenders&gt; &lt;!-- 定义输出到控制台 --&gt; &lt;Console name=\"console\" target=\"SYSTEM_OUT\" follow=\"true\"&gt; &lt;!--控制台只输出level及以上级别的信息--&gt; &lt;ThresholdFilter level=\"DEBUG\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;PatternLayout &gt; &lt;Pattern&gt;$&#123;pattern&#125;&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;/Console&gt; &lt;!-- 同一来源的Appender可以定义多个RollingFile，定义按天存储日志 --&gt; &lt;RollingFile name=\"rolling_file\" fileName=\"$&#123;logDir&#125;/im-server.log\" filePattern=\"$&#123;logDir&#125;/im-server_%d&#123;yyyy-MM-dd&#125;.log\"&gt; &lt;ThresholdFilter level=\"INFO\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;$&#123;pattern&#125;&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy interval=\"1\"/&gt; &lt;/Policies&gt; &lt;!-- 日志保留策略，配置只保留七天 --&gt; &lt;DefaultRolloverStrategy&gt; &lt;Delete basePath=\"$&#123;logDir&#125;/\" maxDepth=\"1\"&gt; &lt;IfFileName glob=\"im-server_*.log\" /&gt; &lt;IfLastModified age=\"7d\" /&gt; &lt;/Delete&gt; &lt;/DefaultRolloverStrategy&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt;&lt;/configuration&gt; 说明 控制台日志高亮设置(IDE为IDEA),在启动主类VM options 中添加一行参数:-Dlog4j.skipJansi=false 在logDir属性配置中可以通过启动类的环境变量(Environment variables)设置IM_HOME路径","categories":[],"tags":[{"name":"Log4J2","slug":"Log4J2","permalink":"https://zhangjunjunah.github.io/tags/Log4J2/"}]},{"title":"Spring AOP结合自定义注解实例","slug":"spring AOP结合自定义注解实例","date":"2020-03-30T16:00:00.000Z","updated":"2020-04-21T02:57:39.846Z","comments":true,"path":"2020/03/31/spring AOP结合自定义注解实例/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/31/spring%20AOP%E7%BB%93%E5%90%88%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E5%AE%9E%E4%BE%8B/","excerpt":"","text":"使用背景​ 近期同事使用产品经常遇到页面点击卡死长时间未响应现象(本人比较擅长解决问题)，心里带着疑问使用jstack命令查看jvm堆栈，发现阻塞的线程都是表更新操作(update)。查看了下代码发现是由于最近大家申请容器资源频繁，申请容器逻辑在一个事务中，又因为资源告警创建时间变长，导致锁表。在这里简单说下申请容器的步骤。 完成资源申请合法校验，更新资源表信息 通过k8sApi创建需要的资源(pod、deployment、job、…) –在资源紧张时创建时间较长 创建失败删除对应的k8s资源 备注：资源表记录了容器平台资源总共多少资源（CPU、GPU、memory），已使用多少等信息 逻辑很简单，但创建容器频繁容易造成锁表，导致页面卡死。改造方案也很简单，更新资源表不走事务，如果创建逻辑失败，手动回滚，不就可以了。代理设计模式最适合这种改造场景，话不多说赶紧动手。 使用实例创建注解123456789101112/*** @Description: 申请容器(服务)资源注解* @Param:* @return:* @Author: zhangjj* @Date: 2020-02-25*/@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface ApplyContainerRes &#123;&#125; 编写切面在这里只放上了相关的代码（包名涉及公司暂时屏蔽了） 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @Description: 申请资源切面 * @ClassName: ApplyContainerResAspect * @Author: zhangjj * @Date: 2020-02-25 */@Aspect@Component@Slf4jpublic class ApplyContainerResAspect &#123; @Autowired private DataSourceTransactionManager dataSourceTransactionManager; @Pointcut(\"@annotation(xx.ApplyContainerRes)\") public void annotationApplyContainerResPointCut()&#123; &#125; @Before(\"annotationApplyContainerResPointCut()\") public void doBefore(JoinPoint joinPoint) &#123; DefaultTransactionDefinition def = new DefaultTransactionDefinition(); //新发起一个事务 def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW); TransactionStatus transaction = dataSourceTransactionManager.getTransaction(def); //伪代码 //申请资源(更新资源表信息) applyRes(joinPoint); dataSourceTransactionManager.commit(transaction); &#125; @AfterThrowing(\"annotationApplyContainerResPointCut()\") public void doAfter(JoinPoint joinPoint) &#123; DefaultTransactionDefinition def = new DefaultTransactionDefinition(); //新发起一个事务 def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW); TransactionStatus transactionStatus = dataSourceTransactionManager.getTransaction(def); //伪代码 //释放资源(更新资源表信息) releaseResource(joinPoint); dataSourceTransactionManager.commit(transactionStatus); &#125;&#125; 业务使用service层业务逻辑样例 12345678910111213141516/*** @Description: 启动服务* @Param: [environmentId,modelServices]* @return: OperContainerResult* @Author: zhangjj* @Date: 2019-08-07*/@Override@ApplyContainerRespublic OperContainerResult startServing(Long environmentId, ModelServices modelServices) &#123; OperContainerResult operContainerResult = new OperContainerResult(); //服务部署 deployModelService(modelServices); operContainerResult.setCode(OperContainerResult.CODE.SUCCESS); return operContainerResult;&#125; 改造完成，测试通过！很顺利嘛(手动怀疑)，可以让同事仿照上面样例改造其他业务逻辑了。 使用中的问题​ 同事的改造速度也很快，本地测试发现不对，资源表的数据怎么没更新啊！我这边心想咋回事，我这边使用都好使，怎么你这却不好使了。。。一起查看逻辑，发现他那的逻辑是在同一个类中调用。我这边意识到动态代理好像不能代理类中方法的直接调用，网上搜搜看看有没有解决方法,很快找到了解决办法。 未调整前代码 1234567891011121314151617181920/** * @param batchWorkId * @Description: 批量作业运行 * @Param: [batchWorkId] * @return: void * @Author: zhangjj * @Date: 2020-01-03 */@Overridepublic void runBatchWork(Long batchWorkId) &#123; //其他业务 dosomething(); //调用被代理的代理类 doRunBatchWork(batchWorkId);&#125;@ApplyContainerRespublic void doRunBatchWork(batchWorkId) &#123; //业务逻辑&#125; 网上的解决方案 既然 doRunBatchWork() 方法调用没有触发 AOP 逻辑的原因是因为我们以目标对象的身份(target object) 来调用的, 那么解决的关键自然就是以代理对象(proxied object)的身份来调用 doRunBatchWork() 方法。 备注:由于这里的调用方法没有声明在接口中，需要将jdk动态代理调整为cglib动态代理(这里先留个坑，以后可以总结下两者的区别) 调整后的代码 12345678910111213141516171819202122232425262728//在springboot启动类上添加下面一行注解@EnableAspectJAutoProxy(exposeProxy=true,proxyTargetClass=true)//业务伪代码@Autowiredprivate XXServiceImpl self;/** * @param batchWorkId * @Description: 批量作业运行 * @Param: [batchWorkId] * @return: void * @Author: zhangjj * @Date: 2020-01-03 */@Overridepublic void runBatchWork(Long batchWorkId) &#123; //其他业务 dosomething(); //调用被代理的代理类 self.doRunBatchWork(batchWorkId);&#125;@ApplyContainerRespublic void doRunBatchWork(batchWorkId) &#123; //业务逻辑&#125; 修改后，代理逻辑生效。不过，没过多久同事又反馈另外一个逻辑这么改造还是不好使，我查看了下发现方法声明是private(被代理的类必须要是public声明)。之前没意识到spring AOP使用中会遇到这些问题，在这里也简单总结下，方便查看回忆。 使用总结 springboot使用cglib代码，在启动类上添加以下注解@EnableAspectJAutoProxy(exposeProxy=true,proxyTargetClass=true) 被代理的方法尽量跨类调用 被代理的方法不能声明为私有方法","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://zhangjunjunah.github.io/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://zhangjunjunah.github.io/tags/Spring/"}]},{"title":"记录一次因内存原因导致k8s创建pod失败问题","slug":"记录一次因内存原因导致k8s创建pod失败问题","date":"2020-03-23T16:00:00.000Z","updated":"2020-04-21T02:57:39.847Z","comments":true,"path":"2020/03/24/记录一次因内存原因导致k8s创建pod失败问题/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/24/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E5%9B%A0%E5%86%85%E5%AD%98%E5%8E%9F%E5%9B%A0%E5%AF%BC%E8%87%B4k8s%E5%88%9B%E5%BB%BApod%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题背景​ 最近同事反馈使用k8s创建jupyter经常在某一台节点创建不起来(其他节点正常)，如果把这台节点的docker重启，当时可以恢复正常，但过段时间问题又会复现,下面附上kubectl describe部分日志。 12345Events: Type Reason Age From Message---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 1s (x13 over 18s) kubelet, centos7-141 Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \"jupyter-lab1584951113027\": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:303: getting the final child's pid from pipe caused \\\"EOF\\\"\": unknown 问题分析​ 带着疑问上网查询报错，有的说是docker版本、centos 内核与k8s版本不兼容导致？ ​ 依次查询了几台主机的docker版本(k8s是一起安装的所以直接排除)，都是19.03.5版本(centos版本也一致)。但仔细想想其他节点都没出现这个问题，可能问题原因不在这，暂时排除。 123456789101112131415161718192021222324252627Client: Docker Engine - Community Version: 19.03.5 API version: 1.40 Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:25:41 2019 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 19.03.5 API version: 1.40 (minimum version 1.12) Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:24:18 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.6 GitCommit: 894b81a4b802e4eb2a91d1ce216b8817763c29fb runc: Version: 1.0.0-rc8 GitCommit: 425e105d5a03fabd737a126ad93d62a9eeede87f docker-init: Version: 0.18.0 GitCommit: fec3683 ​ 问题困扰了一两天，后来看到一篇文章Kubernetes因限制内存配置引发的错误给我启发，会不会是内存原因导致的问题。使用free 命令对比了两台节点的内存。 1234567891011# 问题节点[root@centos7-141 log]# free -g total used free shared buff/cache availableMem: 22 1 0 1 20 12Swap: 0 0 0# 正常节点[root@centos7-142 ~]# free -g total used free shared buff/cache availableMem: 22 2 9 1 10 15Swap: 0 0 0 等等，发现了一些端倪，为啥总计22g，只使用1g,剩余内存为啥是0？还有buff/cache 是啥？带着疑问，查询了 buff/cache,在这记录下。 Linux服务器运行一段时间后，由于其内存管理机制，会将暂时不用的内存转为buff/cache，这样在程序使用到这一部分数据时，能够很快的取出，从而提高系统的运行效率，所以这也正是linux内存管理中非常出色的一点，所以乍一看内存剩余的非常少，但是在程序真正需要内存空间时，linux会将缓存让出给程序使用，这样达到对内存的最充分利用，所以真正剩余的内存是free+buff/cache 在这里按照先解决的原则，按照操作步骤释放了buff/cache。 123456echo 1 &gt; /proc/sys/vm/drop_caches#操作后的内存占用[root@centos7-141 log]# free -g total used free shared buff/cache availableMem: 22 1 7 1 13 12Swap: 0 0 重新创建pod，创建成功，问题解决！ 1234567891011Name: jupyter-lab1584951113027Namespace: kubeflowPriority: 0Node: centos7-141/192.168.128.141Start Time: Tue, 24 Mar 2020 14:00:46 +0800Labels: app=jupyterhub component=singleuser-server heritage=jupyterhubAnnotations: hub.jupyter.org/username: lab1584951113027Status: RunningIP: 10.244.3.3 后记​ 尽管问题解决了，但还是有两个疑问，在这里先记录下来 buff/cache为啥没有在k8s需要内存时及时让出？ 如果buff/cache没有及时让出缓存，为啥k8s events 不提示内存超出限制报错？ 参考 Kubernetes因限制内存配置引发的错误 Linux释放内存空间","categories":[],"tags":[{"name":"K8S","slug":"K8S","permalink":"https://zhangjunjunah.github.io/tags/K8S/"},{"name":"Docker","slug":"Docker","permalink":"https://zhangjunjunah.github.io/tags/Docker/"}]},{"title":"Ceph 配置RGW高可用","slug":"ceph 配置 RGW高可用","date":"2020-03-20T16:00:00.000Z","updated":"2020-04-21T02:57:39.844Z","comments":true,"path":"2020/03/21/ceph 配置 RGW高可用/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/21/ceph%20%E9%85%8D%E7%BD%AE%20RGW%E9%AB%98%E5%8F%AF%E7%94%A8/","excerpt":"","text":"ceph节点规划 节点ip 节点主机名 节点ceph组件 其他相关组件 192.168.153.51 centos 7-ceph-1（ceph 主节点） mds1、mon1、mg1、rgw1、osd1 keepalived 192.168.153.52 centos 7-ceph-2 mon2、mg2、rgw2、osd2 keepalived 192.168.153.53 centos 7-ceph-3 mon3、mg3、osd3 整体架构 新增RGW服务 1234567891011#在centos 7-ceph-1节点使用cephuser用户cd my-clusterceph-deploy rgw create 128# 访问测试curl -I http://centos 7-ceph-1:7480/HTTP/1.1 200 OKx-amz-request-id: tx000000000000000000001-005d0b0e2a-1018-defaultContent-Type: application/xmlContent-Length: 0Date: Thu, 20 Jun 2019 04:40:11 GMT 安装相关软件安装依赖1234#在centos 7-ceph-1、centos 7-ceph-2节点使用root用户执行yum -y install openssl-devel --skip-brokenyum install -y libnl3-devel libnfnetlink-develyum -y install gcc 安装keepalived 12345678910111213141516171819#在centos 7-ceph-1、centos 7-ceph-2节点使用root用户执行cd /usr/local/src#官网下载keepalived的最新版本，解压并安装wget http://www.keepalived.org/software/keepalived-2.0.7.tar.gztar xvf keepalived-2.0.7.tar.gzcd keepalived-2.0.7./configure --prefix=/usr/local/keepalivedmake &amp;&amp; make install#初始化及启动#keepalived启动脚本变量引用文件，默认文件路径是/etc/sysconfig/，也可以不做软链接，直接修改启动脚本中文件路径即可（安装目录下）cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/keepalived # 将keepalived主程序加入到环境变量（安装目录下）cp /usr/local/keepalived/sbin/keepalived /usr/sbin/keepalived# keepalived启动脚本（源码目录下），放到/etc/init.d/目录下就可以使用service命令便捷调用cp /usr/local/src/keepalived-2.0.7/keepalived/etc/init.d/keepalived /etc/init.d/keepalived # 将配置文件放到默认路径下mkdir /etc/keepalivedcp /usr/local/keepalived/etc/keepalived/keepalived.conf etc/keepalived/keepalived.conf keepalived.conf配置12345678910111213141516171819202122232425262728293031#centos 7-ceph-1节点cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bakcat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123;&#125;vrrp_script chk_rgw &#123; script \"/usr/local/keepalived/sbin/check_rgw.sh\" # 该脚本检测rgw的运行状态，并在rgw进程挂了之后尝试重新启动rgw，如果启动失败则停止keepalived，准备让其它机器接管。 interval 2 # 每2s检测一次 weight 2 # 检测失败（脚本返回非0）则优先级2&#125;vrrp_instance VI_1 &#123; state MASTER # 指定keepalived的角色，MASTER表示此主机是主服务器，BACKUP表示此主机是备用服务器 interface ens32 # 指定HA监测网络的接口 根据你实际的网卡名来 keyong ip addr 查询 virtual_router_id 51 # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。即同一vrrp_instance下，MASTER和BACKUP必须是一致的 priority 100 # 定义优先级，数字越大，优先级越高，在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级 advert_int 1 # 设定MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS # 设置验证类型，主要有PASS和AH两种 auth_pass 1111 # 设置验证密码，在同一个vrrp_instance下，MASTER与BACKUP必须使用相同的密码才能正常通信 &#125; virtual_ipaddress &#123; 192.168.153.16 # 设置虚拟IP地址(与节点ip同网段) &#125; track_script &#123; chk_rgw # 引用VRRP脚本，即在 vrrp_script 部分指定的名字。定期运行它们来改变优先级，并最终引发主备切换。 &#125;&#125; 12345678910111213141516171819202122232425262728293031#centos 7-ceph-2节点cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bakcat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123;&#125;vrrp_script chk_rgw &#123; script \"/usr/local/keepalived/sbin/check_rgw.sh\" # 该脚本检测rgw的运行状态，并在rgw进程挂了之后尝试重新启动rgw，如果启动失败则停止keepalived，准备让其它机器接管。 interval 2 # 每2s检测一次 weight 2 # 检测失败（脚本返回非0）则优先级2&#125;vrrp_instance VI_1 &#123; state BACKUP # 指定keepalived的角色，MASTER表示此主机是主服务器，BACKUP表示此主机是备用服务器 interface ens32 # 指定HA监测网络的接口 根据你实际的网卡名来 keyong ip addr 查询 virtual_router_id 51 # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。即同一vrrp_instance下，MASTER和BACKUP必须是一致的 priority 100 # 定义优先级，数字越大，优先级越高，在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级 advert_int 1 # 设定MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS # 设置验证类型，主要有PASS和AH两种 auth_pass 1111 # 设置验证密码，在同一个vrrp_instance下，MASTER与BACKUP必须使用相同的密码才能正常通信 &#125; virtual_ipaddress &#123; 192.168.153.16 # 设置虚拟IP地址(与节点ip同网段) &#125; track_script &#123; chk_rgw # 引用VRRP脚本，即在 vrrp_script 部分指定的名字。定期运行它们来改变优先级，并最终引发主备切换。 &#125;&#125; centos 7-ceph-1/2 中，/usr/local/keepalived/sbin/check_rgw.sh脚本内容如下 1234567891011#!/bin/bashif [ \"$(ps -ef | grep \"radosgw\"| grep -v grep )\" == \"\" ];then systemctl start ceph-radosgw.target sleep 3 if [ \"$(ps -ef | grep \"radosgw\"| grep -v grep )\" == \"\" ];then systemctl stop keepalived fifi#添加check_rgw.sh脚本执行权限chmod +x /usr/local/keepalived/sbin/check_rgw.sh 到这里对keepalived的配置已经完成，然后分别启动centos 7-ceph-1/2点上的keepalived 1systemctl start keepalived 分别在centos 7-ceph-1和centos 7-ceph-2节点上执行ip a命令，查看虚IP信息： 1234567891011121314151617181920212223242526272829#centos 7-ceph-11: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:67:df:45 brd ff:ff:ff:ff:ff:ff inet 192.168.153.51/24 brd 192.168.129.255 scope global noprefixroute ens32 valid_lft forever preferred_lft forever inet 192.168.153.16/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::388c:a9c2:c50b:dd48/64 scope link noprefixroute valid_lft forever preferred_lft forever#centos 7-ceph-21: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:4c:7f:2a brd ff:ff:ff:ff:ff:ff inet 192.168.153.52/24 brd 192.168.129.255 scope global noprefixroute ens32 valid_lft forever preferred_lft forever inet6 fe80::58cc:e799:bc61:7fd5/64 scope link noprefixroute valid_lft forever preferred_lft forever 调整S3配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[cephuser@centos 7-ceph-1 keepalived-2.0.7]$ s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key [AY96HNDL9H2QV2SRPNDJ]: Secret Key [Q8OhUOJdRJg8KzAUwZ22BddL7QbC6g8hHCGwq1jx]: Default Region [US]: Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.S3 Endpoint [192.168.153.51:7480]: 192.168.153.16:7480Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [192.168.153.51:7480]: 192.168.153.16:7480Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: Path to GPG program [/bin/gpg]: When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [No]: On some networks all internet access must go through a HTTP proxy.Try setting it here if you can't connect to S3 directlyHTTP Proxy server name: New settings: Access Key: AY96HNDL9H2QV2SRPNDJ Secret Key: Q8OhUOJdRJg8KzAUwZ22BddL7QbC6g8hHCGwq1jx Default Region: US S3 Endpoint: 192.168.153.16:7480 DNS-style bucket+hostname:port template for accessing a bucket: 192.168.153.16:7480 Encryption password: Path to GPG program: /bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0Test access with supplied credentials? [Y/n] yPlease wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] yConfiguration saved to '/home/cephuser/.s3cfg'[cephuser@centos 7-ceph-1 keepalived-2.0.7]$ s3cmd ls2020-03-11 07:16 s3://MyBucket_12020-03-11 07:45 s3://algorithm-bucket2020-03-11 07:44 s3://demo-bucket 参考引用keepalived配置RGW高可用","categories":[],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://zhangjunjunah.github.io/tags/Ceph/"},{"name":"Linux","slug":"Linux","permalink":"https://zhangjunjunah.github.io/tags/Linux/"}]},{"title":"Ceph和s3的安装","slug":"ceph和s3的安装","date":"2020-03-20T16:00:00.000Z","updated":"2020-04-21T02:57:39.845Z","comments":true,"path":"2020/03/21/ceph和s3的安装/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/21/ceph%E5%92%8Cs3%E7%9A%84%E5%AE%89%E8%A3%85/","excerpt":"","text":"部署说明 安装的ceph版本（14.2.4）、s3 版本(2.0.2) ceph使用ceph-deploy部署高可用集群至少需要3台节点(ceph 监视器需要3个以上节点) ceph推荐使用裸盘安装osd(文件夹也可以部署，但不推荐) ceph部署节点规划 节点ip 节点主机名 节点ceph组件 192.168.153.51 centos 7-ceph-1（ceph 主节点） mds1、mon1、mg1、rgw、osd1 192.168.153.52 centos 7-ceph-2 mon2、mg2、osd2、 192.168.153.53 centos 7-ceph-3 mon3、mg3、osd3 创建部署用户注意，在外网环境，禁止使用 ceph 服务名 作为用户，防止暴力破解。 因为 ceph-deploy 部署工具需要以登录用户（且包含sudo权限）来安装软件和做配置，因此通常在主节点创建 ceph 用户。 12345678910111213141516171819# 在每一个节点执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）useradd cephuserecho 'cephuser' | passwd --stdin cephuserecho \"cephuser ALL = (root) NOPASSWD:ALL\" &gt; /etc/sudoers.d/cephuserchmod 0440 /etc/sudoers.d/cephuser# 配置sshd可以使用password登录sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_configsystemctl reload sshd# 配置sudo不需要ttysed -i 's/Default requiretty/#Default requiretty/' /etc/sudoers#每个节点添加hosts192.168.153.51 centos 7-ceph-1192.168.153.52 centos 7-ceph-2192.168.153.53 centos 7-ceph-3# 设置免密登录（使用cephuser用户）ssh-keygen -t rsa # 在centos 7-ceph-1使用cephuser用户操作ssh-copy-id -i /home/cephuser/.ssh/id_rsa.pub cephuser@centos 7-ceph-2ssh-copy-id -i /home/cephuser/.ssh/id_rsa.pub cephuser@centos 7-ceph-3 创建虚拟卷通过虚拟机管理界面分别为每个节点 挂载新磁盘 12345# 每台节点操作(root)fdisk -l #查看挂载的磁盘名，#格式化磁盘parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%mkfs.xfs /dev/sdb -f 关闭 SetLinux（建议） 12345# 建议执行，注意需要重启机器setenforce 0 sed -i \"s/SELINUX=enforcing/SELINUX=disabled /g\" /etc/selinux/config # 重启机器，检查，如果SELinux status参数为disabled即为关闭状态/usr/sbin/sestatus -v 关闭防火墙 123# 可选，如果不关闭需要开启对应端口，本例简单的关闭处理systemctl disable firewalld.service systemctl stop firewalld.service 配置ntp服务器 12345678910111213141516# 在三台节点root执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）yum -y install ntp# 将centos 7-ceph-1配置成ntp服务端[root@centos7-ceph-1 ~]# cat /etc/ntp.confserver 127.127.1.0fudge 127.127.1.0 stratum 10restrict 192.168.153.1 mask 255.255.255.0 nomodify notrap#启动ntpsystemctl start ntpd &amp;&amp; systemctl enable ntpd# centos 7-ceph-2、centos 7-ceph-3配置ntp客户端cat /etc/ntp.conf server 192.168.153.51#执行手动同步ntpdate 192.168.153.51systemctl start ntpd &amp;&amp; systemctl enable ntpd 开始部署安装 ceph 依赖 12345# ceph-deploy 是ceph 的部署工具，在管理节点(centos 7-ceph-1执行)yum install -y ceph-deploy ceph-deploy --version // 2.0.1# 安装 ceph 和 radosgw ,在每台节点执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）yum install -y ceph ceph-radosgw 搭建 ceph 服务 12345678910111213141516171819202122232425262728293031323334# 注意以下所有使用 ceph 用户执行，不要使用 sudo ，不要使用rootsu - cephusermkdir my-clustercd my-cluster# 创建集群，用 new 命令，并指定几个主机安装初始ceph Monitor 服务# 在centos 7-ceph-1上操作ceph-deploy new centos 7-ceph-1# 此时会在当前目录生成 ceph 配置文件、日志和密钥文件# 注意实际生效的配置文件将在 /etc/ceph/ceph.conf （当前还未生生成），最佳实践为修改 $HOME 目录下的文件配置，通过 deploy 工具分发到各节点lsceph.conf ceph-deploy-ceph.log ceph.mon.keyring# 需要修改一下配置文件echo \"public network = 192.168.153.0/24\" &gt;&gt; ceph.confecho \"cluster network = 192.168.153.0/24\" &gt;&gt; ceph.confecho \"osd pool default size = 3\" &gt;&gt; ceph.confecho \"osd pool default min size = 2\" &gt;&gt; ceph.confecho \"mon_max_pg_per_osd = 650\" &gt;&gt; ceph.confecho \"[osd]\" &gt;&gt; ceph.confecho \"mon_osd_backfillfull_ratio=0.7\" &gt;&gt; ceph.confecho \"mon_osd_full_ratio=0.8\" &gt;&gt; ceph.confecho \"mon_osd_nearfull_ratio=0.6\" &gt;&gt; ceph.confecho \"osd_failsafe_full_ratio=0.8\" &gt;&gt; ceph.conf# 部署monitor和生成keysceph-deploy mon create-initialls -l *.keyring# 复制文件到node节点ceph-deploy --overwrite-conf admin centos 7-ceph-1 centos 7-ceph-2 centos 7-ceph-3# 部署manager （luminous+）12及以后的版本需要部署ceph-deploy mgr create centos 7-ceph-1 创建 OSD（对象存储后台进程） 1234567891011#在centos 7-ceph-1用cephuser用户执行ceph-deploy osd create --data /dev/sdb centos 7-ceph-1 ceph-deploy osd create --data /dev/sdb centos 7-ceph-2 ceph-deploy osd create --data /dev/sdb centos 7-ceph-3 # 查看 osd tree和ceph集群状态sudo ceph osd treesudo ceph -ssudo ceph health detail //HEALTH_OK# 至此 基本ceph安装完毕 高可用配置(mon、mgr) 123456789# 添加monitorceph-deploy mon add centos 7-ceph-2ceph-deploy mon add centos 7-ceph-3# 添加managerceph-deploy mgr create centos 7-ceph-2 centos 7-ceph-3# 卸载monitorceph-deploy mon destroy centos 7-ceph-2 部署RGWCeph部署RGW以兼容S3/Swift接口，即可以使用S3或Swift的命令行工具或SDK来使用ceph。 部署RGW网关 12345678910111213141516171819202122232425# 启动 RGW，注意开放的网关端口，默认7480ceph-deploy rgw create centos 7-ceph-1 // The Ceph Object Gateway (RGW) is now running on host centos 7-ceph-1 and default port 7480# 访问测试curl -I http://centos 7-ceph-1:7480/HTTP/1.1 200 OKx-amz-request-id: tx000000000000000000001-005d0b0e2a-1018-defaultContent-Type: application/xmlContent-Length: 0Date: Thu, 20 Jun 2019 04:40:11 GMT# 页面访问测试 # 至此 Ceph 端对象访问接口已经部署完成，可使用s3 工具访问 # 附其他操作 # 附1：更改开放端口，需重启RGW# 修改配置 /etc/ceph/ceph.conf，使用 rgw 监听在 80 端口 [client.rgw.lab1]rgw_frontends = \"civetweb port=80\" # 附2：重启 RGWsystemctl restart ceph-radosgw@rgw.k8s113 对象存储服务与测试（S3/Swift）离线安装 s3cmd 12345rpm -ivh python-dateutil-1.5-7.el7.noarch.rpm python-magic-5.11-35.el7.noarch.rpm s3cmd-2.0.2-1.el7.noarch.rpms3cmd --versions3cmd version 2.0.2 对象存储测试（使用s3cmd测试） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141# 测试# 创建S3所需要的poolsudo ceph osd pool create .rgw 128 128sudo ceph osd pool create .rgw.root 128 128sudo ceph osd pool create .rgw.control 128 128sudo ceph osd pool create .rgw.gc 128 128sudo ceph osd pool create .rgw.buckets 128 128sudo ceph osd pool create .rgw.buckets.index 128 128sudo ceph osd pool create .rgw.buckets.extra 128 128sudo ceph osd pool create .log 128 128sudo ceph osd pool create .intent-log 128 128sudo ceph osd pool create .usage 128 128sudo ceph osd pool create .users 128 128sudo ceph osd pool create .users.email 128 128sudo ceph osd pool create .users.swift 128 128sudo ceph osd pool create .users.uid 128 128# 查看rados lspools# 访问测试curl -I http://192.168.153.51:7480/# 创建S3用户# 注意：保存命令返回的 user access_key secret_keysudo radosgw-admin user create --uid=s3_user --display-name=s3_user --email=s3@s3.com\"access_key\": \"FZ95GV7T0WU8OVOL4YH9\",\"secret_key\": \"WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPK\"# 创建admin用户radosgw-admin user create --uid=admin --display-name=admin\"access_key\": \"B3M00AHVE9FEXCQ8D0L9\",\"secret_key\": \"JVJG1O4LDGam35EBghXGsgrdOwhqGzVKPtR6ntw4\"# 允许admin读写所有users信息radosgw-admin caps add --uid=admin --caps=\"users=*\"# 允许admin读写所有的usage信息radosgw-admin caps add --uid=admin --caps=\"usage=read,write\"# 配置s3cmd（当前在 ceph目录下，附完整过程，输入之前创建的 s3_user_tydic 用户密钥） s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key: FZ95GV7T0WU8OVOL4YH9Secret Key: WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPKDefault Region [US]:Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.S3 Endpoint [s3.amazonaws.com]: 192.168.153.51:7480 //注意：此处必须使用 ip:port 形式，使用域名方式将会导致从s3cmd创建的桶必须是以大写字母开头，此命名与亚马逊官方桶命名要求相悖，建议修改为ip:port 方式。Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 192.168.153.51:7480 //ip:port 形式Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: //空Path to GPG program [/bin/gpg]: //空When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [Yes]: noOn some networks all internet access must go through a HTTP proxy.Try setting it here if you can't connect to S3 directlyHTTP Proxy server name: //空Test access with supplied credentials? [Y/n] yPlease wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] yConfiguration saved to '/home/ceph/.s3cfg'# 注意 s3 配置文件生成位置修该生成的配置文件，后续可做自定义修改# 重点配置项示例，其他项均为默认项[default]access_key = FZ95GV7T0WU8OVOL4YH9secret_key = WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPKhost_base = 192.168.153.51:7480host_bucket = 192.168.153.51:7480use_https = False# 创建Bucket（注意命令行端bucket首字母必须大写）s3cmd mb s3://MyBucket_1s3cmd ls# 上传Objectecho 'hello ceph block storage s3' &gt; hello.txts3cmd put hello.txt s3://MyBucket_1# 查看Objects3cmd ls s3://MyBucket_1# 下载Objectcd /tmps3cmd get s3://mybucket/hello.txtcat hello.txt# 删除bucket下所有对象s3cmd del -rf s3://MyBucket_1 s3cmd ls -r s3://MyBucket_1# 删除Buckets3cmd mb s3://MyBucket_1s3cmd rb s3://MyBucket_1# 其他操作# 删除S3用户radosgw-admin user rm --uid=s3_user_tydicradosgw-admin user rm --uid=admin#调整pool副本数ceph osd pool set default.rgw.buckets.data size 3ceph osd pool set default.rgw.buckets.data min_size 2# 删除poolceph osd pool delete .rgw .rgw --yes-i-really-really-mean-itceph osd pool delete .rgw.root .rgw.root --yes-i-really-really-mean-itceph osd pool delete .rgw.control .rgw.control --yes-i-really-really-mean-itceph osd pool delete .rgw.gc .rgw.gc --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets .rgw.buckets --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.index .rgw.buckets.index --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.extra .rgw.buckets.extra --yes-i-really-really-mean-itceph osd pool delete .log .log --yes-i-really-really-mean-itceph osd pool delete .intent-log .intent-log --yes-i-really-really-mean-itceph osd pool delete .usage .usage --yes-i-really-really-mean-itceph osd pool delete .users .users --yes-i-really-really-mean-itceph osd pool delete .users.email .users.email --yes-i-really-really-mean-itceph osd pool delete .users.swift .users.swift --yes-i-really-really-mean-itceph osd pool delete .users.uid .users.uid --yes-i-really-really-mean-it Ceph DashboardCeph Dashboard介绍Ceph 的监控可视化界面方案很多—-grafana、Kraken。但是从Luminous开始，Ceph 提供了原生的Dashboard功能，通过Dashboard可以获取Ceph集群的各种基本状态信息。 配置Ceph Dashboard安装和配置12345678910111213141516171819202122# 1、在每个mgr节点安装 yum install ceph-mgr-dashboard # 2、开启mgr功能 ceph mgr module enable dashboard # 3、生成并安装自签名的证书 ceph dashboard create-self-signed-cert # 4、创建一个dashboard登录用户名密码 ceph dashboard ac-user-create super 123456 administrator # 5、查看服务访问方式 ceph mgr services&#123; \"dashboard\": \"https://centos 7-ceph-1:8443/\"&#125;#安装完成#可选#修改ipsudo ceph config set mgr mgr/dashboard/server_addr 192.168.153.51#修改端口sudo ceph config set mgr mgr/dashboard/server_port 8443#禁用httpssudo ceph config set mgr mgr/dashboard/ssl false 安装问题汇总1.格式化磁盘时提示忙 1mkfs.xfs: cannot open /dev/sdb: Device or resource busy 解决方法 123456789lsblk #显示部分磁盘正常，部分下面有-ceph-**等标识，用ilo多次格式化磁盘作raid0均无效果dmsetup ls #查看谁在占用，找到ceph-**字样（ceph-**为lsblk显示的块设备具体信息）#使用dmsetup 删除字样dmsetup remove ceph-**lsblk #查看设备信息，可以看到ceph-**等标识等标识消失ceph-deploy disk zap centos7-ceph-3 /dev/sdb #格式化磁盘sudo parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%sudo mkfs.xfs /dev/sdb -f 2.ceph-deploy new centos 7-ceph-1 时，提示 ImportError: No module named pkg_resources 1234567[cephuser@centos 7-ceph-1 my-cluster]$ ceph-deploy new centos 7-ceph-1Traceback (most recent call last): File \"/bin/ceph-deploy\", line 18, in &lt;module&gt; from ceph_deploy.cli import main File \"/usr/lib/python2.7/site-packages/ceph_deploy/cli.py\", line 1, in &lt;module&gt; import pkg_resourcesImportError: No module named pkg_resources 解决方法 12345#安装pipsudo yum -y install epel-releasesudo yum -y install python-pip#安装distribute类库pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com distrbute 3.如果安装出错如何清理 12345# 卸载Ceph安装包ceph-deploy purge &lt;hostname&gt;# 清理配置ceph-deploy purgedata &lt;hostname&gt;ceph-deploy forgetkeys 安装ceph dashboard 提示 No module 1Module 'dashboard' has failed dependency: No module named urllib3.exceptions 解决方法 12#pip安装 urllib3pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com urllib3","categories":[],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://zhangjunjunah.github.io/tags/Ceph/"},{"name":"Linux","slug":"Linux","permalink":"https://zhangjunjunah.github.io/tags/Linux/"}]}]}