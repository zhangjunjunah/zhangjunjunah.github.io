{"meta":{"title":"机锋小摩托的分享","subtitle":"","description":"个人学习及工作经验分享","author":"机锋小摩托","url":"https://zhangjunjunah.github.io","root":"/"},"pages":[{"title":"关于","date":"2020-03-22T01:24:33.077Z","updated":"2020-03-21T10:26:01.350Z","comments":false,"path":"about/index.html","permalink":"https://zhangjunjunah.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2020-03-22T02:26:39.002Z","updated":"2020-03-22T02:26:39.002Z","comments":false,"path":"books/index.html","permalink":"https://zhangjunjunah.github.io/books/index.html","excerpt":"","text":"Effective Java Java 编程思想"},{"title":"分类","date":"2020-03-21T10:26:01.351Z","updated":"2020-03-21T10:26:01.351Z","comments":false,"path":"categories/index.html","permalink":"https://zhangjunjunah.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-03-22T02:29:57.368Z","updated":"2020-03-22T02:29:57.368Z","comments":false,"path":"tags/index.html","permalink":"https://zhangjunjunah.github.io/tags/index.html","excerpt":"","text":"ceph"}],"posts":[{"title":"jupyterlab个性化开发及配置(初级篇)","slug":"jupyterlab个性化开发及配置-初级篇","date":"2021-12-29T02:06:57.000Z","updated":"2022-02-12T04:00:10.542Z","comments":true,"path":"2021/12/29/jupyterlab个性化开发及配置-初级篇/","link":"","permalink":"https://zhangjunjunah.github.io/2021/12/29/jupyterlab%E4%B8%AA%E6%80%A7%E5%8C%96%E5%BC%80%E5%8F%91%E5%8F%8A%E9%85%8D%E7%BD%AE-%E5%88%9D%E7%BA%A7%E7%AF%87/","excerpt":"","text":"1.个性化背景​ 前段时间，在某个私有环境，我们部署的jupyterlab容器准备正式开始使用。在使用之前，产品经理又提出了两处安全隐患。 ​ jupyterlab容器里的文件可以直接下载 ​ jupyterlab容器里可以直接上传文件 上线前需要将这两类(上传/下载)功能屏蔽。我想这应该问题不大，经过几个小时尝试后，顺利完成了以上漏洞的修复，下面是具体的配置漏洞,在此记录下。 2.个性化修改及配置步骤1.下载jupyterlab源码，确定需要修改代码的位置。 通过F12得到【下载】按钮的data-command为filebrowser:download 2.将需要屏蔽的按钮注释，修改的代码在项目文件夹packages中 3.执行编译命令 12345pip install -e . -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.comjlpm installjlpm run build # Build the dev mode assets (optional)jupyter lab --dev-mode #查看修改效果jupyter lab build # Build the app dir assets (optional) 4.将在dev_mode下的static编译好的文件，替换到jupyterlab的安装目录 备注: 如果仅仅是不让右击按钮不显示，直接修改jupytarlab下的.json文件即可。 3.总结与参考如果只是修改屏蔽些菜单，处理起来并不复杂。后续如果需要修改jupyter功能，还需要多学习ts的相关语法。 [1]jupyterlab使用教程","categories":[],"tags":[{"name":"jupyterlab","slug":"jupyterlab","permalink":"https://zhangjunjunah.github.io/tags/jupyterlab/"}]},{"title":"linux .bashrc ~/.bash_profile傻傻分不清","slug":"linux-bashrc-bash-profile傻傻分不清","date":"2021-12-10T02:19:52.000Z","updated":"2021-12-10T02:19:52.488Z","comments":true,"path":"2021/12/10/linux-bashrc-bash-profile傻傻分不清/","link":"","permalink":"https://zhangjunjunah.github.io/2021/12/10/linux-bashrc-bash-profile%E5%82%BB%E5%82%BB%E5%88%86%E4%B8%8D%E6%B8%85/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"linux buff/cache的那些事儿","slug":"linux buffcache的那些事儿","date":"2021-12-10T01:57:02.000Z","updated":"2021-12-10T01:57:02.339Z","comments":true,"path":"2021/12/10/linux buffcache的那些事儿/","link":"","permalink":"https://zhangjunjunah.github.io/2021/12/10/linux%20buffcache%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Nginx location斜杠配置小结","slug":"Nginx location斜杠配置小结","date":"2021-10-17T12:30:41.000Z","updated":"2021-11-08T09:22:13.734Z","comments":true,"path":"2021/10/17/Nginx location斜杠配置小结/","link":"","permalink":"https://zhangjunjunah.github.io/2021/10/17/Nginx%20location%E6%96%9C%E6%9D%A0%E9%85%8D%E7%BD%AE%E5%B0%8F%E7%BB%93/","excerpt":"","text":"问题背景​ “奇怪了师傅，为啥这个网页直接用浏览器可以访问，但放到我们的产品里怎么就提示‘网页无法访问了’？”新同事 ​ 我:”凑过身子(打开F12看了iframe的url),你这个是跨域访问啊。两个网站不是相同的ip+port，浏览器的安全策略不允许直接访问。” ​ 新同事：”啊，这怎么办。按需求这个网页要在我们的产品里访问。“ ​ 我:”好办，你还记得我之前跟你说的nginx吗？nginx主要有两个功能(正向/反向代理)，我们这里用它的反向代理功能。” 说完就配置了起来，”咦，为啥pass pass加了/访问404？“,”耶，怎么location 后面不加/后面的加载的css和js没带请求前缀？”。”可以了可以了，这里的弯弯绕这么多啊“，坑之坑之半拉小时，两人终于配置完成(多次试错)。 ​ 作为一位工作多年的程序员，nginx location配置不熟练实在不应该，所以自己记录总结下。 配置过程​ 先简单介绍下几个概念 跨域访问​ 跨域，指的是浏览器不能执行其他网站的脚本。它是由浏览器的同源策略造成的，是浏览器施加的安全限制。 正向代理​ 正向代理类似一个跳板机，代理访问外部资源 比如我们国内访问谷歌，直接访问访问不到，我们可以通过一个正向代理服务器，请求发到代理服，代理服务器能够访问谷歌，这样由代理去谷歌取到返回数据，再返回给我们，这样我们就能访问谷歌了 ![img](E:\\workspaces\\self blog\\source_posts\\Nginx location斜杠配置小结\\正向代理.png) 正向代理的用途： （1）访问原来无法访问的资源，如google ​ （2） 可以做缓存，加速访问资源 （3）对客户端访问授权，上网进行认证 （4）代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息 反向代理反向代理（Reverse Proxy）实际运行方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器 ![](E:\\workspaces\\self blog\\source_posts\\Nginx location斜杠配置小结\\反向代理.png) 反向代理的作用： （1）保证内网的安全，阻止web攻击，大型网站，通常将反向代理作为公网访问地址，Web服务器是内网 （2）负载均衡，通过反向代理服务器来优化网站的负载 （3）跨域访问 location与proxy_pass测试​ 正题开始，下面来进行location /配置总结 ​ 前置条件：zeppelin初始访问地址:http://192.168.129.121:7070 测试过程 location不带/,proxy_pass 不带/: 123location /zeppelin &#123; proxy_pass http://192.168.129.121:7070; &#125; 访问地址：http://192.168.129.121:8100/zeppelin 结果：![image-20211108102313083](D:\\workspaces-personal\\self blog\\source_posts\\Nginx location斜杠配置小结\\zeppelin-404.png) location带/,proxy_pass 不带/: 123location /zeppelin/ &#123; proxy_pass http://192.168.129.121:7070; &#125; 访问地址：http://192.168.129.121:8100/zeppelin 结果：![image-20211108102313083](D:\\workspaces-personal\\self blog\\source_posts\\Nginx location斜杠配置小结\\zeppelin-404.png) 3.location不带/,proxy_pass 带/: 123location /zeppelin &#123; proxy_pass http://192.168.129.121:7070/; &#125; ​ 结果：![image-20211108143306385](D:\\workspaces-personal\\self blog\\source_posts\\Nginx location斜杠配置小结\\zeppelin-loading.png) ​ 4.location带/,proxy_pass 带/: 123location /zeppelin/ &#123; proxy_pass http://192.168.129.121:7070/; &#125; ​ 结果：正常访问 访问配置记录proxy_pass不带/都访问不了;location不带斜杠，静态资源请求会因为不带location部分而提示404。 咱们再继续深入分析原因。 情况分析请求访问路径:http://192.168.129.121:8100/zeppelin情况一：proxy_pass不带/，location不带/ ,代理后路径:http://192.168.129.121:7070/zeppelin，访问`404`情况二: proxy_pass不带/，location带/,代理后路径:http://192.168.129.121:7070/zeppelin，访问`404`情况三:proxy_pass带/，location不带/ ,代理后路径:http://192.168.129.121:7070/，访问正常，但其后的静态资源访问`404`，转发后的资源不带`/zeppelin`情况四：proxy_pass带/，location带/ ,代理后路径:http://192.168.129.121:7070/，访问正常.转发后的资源带`/zeppelin` 小结Nginx的官网将proxy_pass分为两种类型：一种是只包含IP和端口号的（连端口之后的/也没有，这里要特别注意），比如proxy_pass http://192.168.129.121:7070，这种方式称为不带URI方式；另一种是在端口号之后有其他路径的，包含了只有单个/的如proxy_pass http://192.168.129.121:7070/，以及其他路径，比如proxy_pass http://192.168.129.121:7070/zeppelin。 也即：proxy_pass http://192.168.129.121:7070和proxy_pass http://192.168.129.121:7070/(多了末尾的/)是不同的的处理方式，而proxy_pass http://192.168.129.121:7070/和proxy_pass http://192.168.129.121:7070/abc是相同的处理方式。 情况三的问题，后续再研究。 参考链接[1]Nginx中proxy_pass的斜杠问题","categories":[],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://zhangjunjunah.github.io/tags/Nginx/"}]},{"title":"Linux文件系统小结","slug":"Linux文件系统小结","date":"2021-06-26T08:08:41.000Z","updated":"2021-06-30T07:03:41.328Z","comments":true,"path":"2021/06/26/Linux文件系统小结/","link":"","permalink":"https://zhangjunjunah.github.io/2021/06/26/Linux%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%B0%8F%E7%BB%93/","excerpt":"","text":"背景​ 最近，公司需要再客户环境新搭一套k8s+ceph环境，主机只有一个操作系统，磁盘未处理。部门的同事都挺忙的，那主机初始化的工作也就我来做吧(名副其实打杂程序员)，正好借这个机会整理下linux文件系统。 操作步骤（虚拟机模拟）前置条件: 通过虚拟机新增了一块硬盘。 方案一:直接分区1.1查看磁盘信息 通过lsblk或fdisk -l 可以查看到磁盘挂载信息。 1.2为sdb磁盘分区命令行输入 fdisk /dev/sdb [sdb为你新添磁盘名称],开始对磁盘的进行设置 1.3使用 mkfs 创建文件系统12345678910111213141516171819202122(base) [root@centos ~]# mkfs.ext4 /dev/sdb1mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks1310720 inodes, 5242624 blocks262131 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=2153775104160 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done 1.4挂载新分区到目录1mount /dev/sdb1 /data 1.5查看挂载信息 方案二:通过LVM逻辑卷管理2.3创建pv pvcreate(前面步骤与一相同)12345678910111213141516171819202122232425262728[root@centos ~]# pvcreate /dev/sdb1WARNING: ext4 signature detected on /dev/sdb1 at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/sdb1. Physical volume \"/dev/sdb1\" successfully created. [root@centos ~]# pvdisplay --- Physical volume --- PV Name /dev/sda2 VG Name centos PV Size 49.80 GiB / not usable 3.00 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 12749 Free PE 0 Allocated PE 12749 PV UUID fbIzRU-PkAg-2SWx-VEkM-t0Sk-FsRe-i8yBIM \"/dev/sdb1\" is a new physical volume of \"&lt;20.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdb1 VG Name PV Size &lt;20.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID QEBpPz-K6MQ-0ku9-iXdw-rA1D-Bth2-LSdc4d 2.4创建vg vgcreate1234567891011121314151617181920212223(base) [root@centos ~]# vgcreate vg_test /dev/sdb1 Volume group \"vg_test\" successfully created(base) [root@centos ~]# vgdisplay --- Volume group --- VG Name vg_test System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size &lt;20.00 GiB PE Size 4.00 MiB Total PE 5119 Alloc PE / Size 0 / 0 Free PE / Size 5119 / &lt;20.00 GiB VG UUID cTHsWq-VxJv-ORbf-uI2A-FQml-fbdq-77gC0p 2.5创建lv lvcreate1234567891011121314151617181920[root@centos ~]# lvcreate -l 100%VG -n lv_test vg_test Logical volume \"lv_test\" created.[root@centos ~]# lvdisplay --- Logical volume --- LV Path /dev/vg_test/lv_test LV Name lv_test VG Name vg_test LV UUID JdTeKJ-5kj3-pide-YVZe-hAzy-aqAX-eKVK0J LV Write Access read/write LV Creation host, time centos, 2021-06-26 20:04:17 +0800 LV Status available # open 0 LV Size &lt;20.00 GiB Current LE 5119 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:4 2.6格式化并挂载123456789101112131415161718192021222324(base) [root@centos ~]# mkfs -t ext4 /dev/vg_test/lv_testmke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks1310720 inodes, 5241856 blocks262092 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=2153775104160 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done (base) [root@centos ~]# mount /dev/vg_test/lv_test /data 2.7查看挂载信息 LVM概念及方案比较逻辑盘卷管理 LVM是逻辑盘卷管理（Logical Volume Manager）的简称，它是Linux环境下对磁盘分区进行管理的一种机制，LVM是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。LVM的工作原理其实很简单，它就是通过将底层的物理硬盘抽象的封装起来，然后以逻辑卷的方式呈现给上层应用。在传统的磁盘管理机制中，我们的上层应用是直接访问文件系统，从而对底层的物理硬盘进行读取，而在LVM中，其通过对底层的硬盘进行封装，当我们对底层的物理硬盘进行操作时，其不再是针对于分区进行操作，而是通过一个叫做逻辑卷的东西来对其进行底层的磁盘管理操作。比如说我增加一个物理硬盘，这个时候上层的服务是感觉不到的，因为呈现给上层服务的是以逻辑卷的方式。 LVM最大的特点就是可以对磁盘进行动态管理。因为逻辑卷的大小是可以动态调整的，而且不会丢失现有的数据。如果我们新增加了硬盘，其也不会改变现有上层的逻辑卷。作为一个动态磁盘管理机制，逻辑卷技术大大提高了磁盘管理的灵活性。PV（Physical Volume）-物理卷物理卷在逻辑卷管理中处于最底层，它可以是实际物理硬盘上的分区，也可以是整个物理硬盘，也可以是raid设备。VG（Volumne Group）-卷组卷组建立在物理卷之上，一个卷组中至少要包括一个物理卷，在卷组建立之后可动态添加物理卷到卷组中。一个逻辑卷管理系统工程中可以只有一个卷组，也可以拥有多个卷组。LV（Logical Volume）-逻辑卷逻辑卷建立在卷组之上，卷组中的未分配空间可以用于建立新的逻辑卷，逻辑卷建立后可以动态地扩展和缩小空间。系统中的多个逻辑卷可以属于同一个卷组，也可以属于不同的多个卷组。 LVM使用分层结构，如下图所示。 图中顶部，首先是实际的物理磁盘及其划分的分区和其上的物理卷（PV）。一个或多个物理卷可以用来创建卷组（VG）。然后基于卷组可以创建逻辑卷（LV）。只要在卷组中有可用空间，就可以随心所欲的创建逻辑卷。文件系统就是在逻辑卷上创建的，然后可以在操作系统挂载和访问。 对比 传统分区使用固定大小分区，重新调整大小十分麻烦。但是，LVM可以创建和管理“逻辑”卷，而不是直接使用物理硬盘。可以让管理员弹性的管理逻辑卷的扩大缩小，操作简单，而不损坏已存储的数据。可以随意将新的硬盘添加到LVM，以直接扩展已经存在的逻辑卷。LVM并不需要重启就可以让内核知道分区的存在。 命令总结一、Linux磁盘分区与文件系统类命令 命令 说明 df 检查文件系统的磁盘空间占用情况，参数-a列出全部目录，参数-h按KB,MB,GB显示 du 检测某个目录或文件占用磁盘的空间，参数-s显示占用总空间；参数-sh统计目录大小 mount 挂载各种文件系统，如mount -t &lt;**文件系统**&gt; 设备名 挂载点 file 判断文件类型 parted 适合与MBR、GPT两种模式的分区命令 fdisk 用于磁盘分区，是Linux的磁盘分区表操作工具 mkfs 磁盘格式化 fsck fsck用来检查和维护不一致的文件系统 二、LVM命令 常用 功能/**命令 物理卷管理 卷组管理 逻辑卷管理 扫描 pvscan vgscan lvscan 建立 pvcreate vgcreate lvcreate 显示 pvdisplay vgdisplay lvdisplay 删除 pvremove vgremove lvremove 扩展 vgextend lvextend 缩小 vgreduce lvreduce 命令 说明 pvcreate pvcreate 设备全路径名，物理卷的创建 pvremove 物理卷的删除 vgscan 检测系统中所有磁盘 vgck vgck [卷组名]检测卷组中卷组描述区域信息的一致性。 vgdisplay vgdisplay [卷组名] 显示卷组的属性信息 vgrename vgrename 原卷组名 新卷组名 vgchange #vgchange -a y|n [卷组名] //改变卷组的相应属性，是否可分配#vgchange -l 最大逻辑卷数 //卷组可容纳最大逻辑卷数#vgchange -x y|n [卷组名] //卷是否有效 vgexport 卷组的输入 vgimport 卷组的输出 pvs #显示PV的显示信息 vgcfgbackup vgcfgbackup [卷组名] //把卷组中VGDA信息备份到”/etc/vmconf”目录中的文件 vgcfgrestore vgcfgrestore -n [卷组名] 物理卷全路径名 //从备份文件中得到指定物理卷的信息 vgcreate 卷组创建 vgremove 卷组删除 vgextend 卷组扩展 vgreduce 卷组缩小 vgmerge 卷组合并 vgsplit 卷组拆分 vgs 显示VG简要信息 lvcreate 逻辑卷创建 lvremove 逻辑卷删除 lvextend 逻辑卷扩展 lvreduce 逻辑卷缩小 lvmdiskscan 检查所有的SCSI,IDE等存储设备 lvscan 检测逻辑卷的状态 lvdisplay 显示LV属性信息 lvchange 修改LV属性信息，-ay:标记LV为可用状态， -an：标记lv为不可用状态 lvs 显示LV 简要信息 参考引用[1][linux分区满了，如何进行扩容] [2][Linux之挂载新的硬盘(超详细!)] [3][Linux系统下创建LV（逻辑卷）并挂载] [4][3分钟看懂linux磁盘划分] [5][基于LVM的磁盘管理] [6][Linux磁盘分区与LVM详解]","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zhangjunjunah.github.io/tags/Linux/"}]},{"title":"markdown转网页小结","slug":"markdown转网页小结","date":"2021-06-24T07:03:08.000Z","updated":"2021-06-24T07:03:08.271Z","comments":true,"path":"2021/06/24/markdown转网页小结/","link":"","permalink":"https://zhangjunjunah.github.io/2021/06/24/markdown%E8%BD%AC%E7%BD%91%E9%A1%B5%E5%B0%8F%E7%BB%93/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"zeppelin 0.8.0 适配FusionInsight_HD_6.5.1.7","slug":"zeppelin-0-8-0-适配FusionInsight-HD-6-5-1-7","date":"2020-12-03T16:00:00.000Z","updated":"2020-12-04T09:38:52.847Z","comments":true,"path":"2020/12/04/zeppelin-0-8-0-适配FusionInsight-HD-6-5-1-7/","link":"","permalink":"https://zhangjunjunah.github.io/2020/12/04/zeppelin-0-8-0-%E9%80%82%E9%85%8DFusionInsight-HD-6-5-1-7/","excerpt":"","text":"适配背景​ 最近客户的华为集群升级到6.5.1.7，我们安装的zeppelin又不好使了。。。 适配方法​ 适配方法参考一篇blog，在这里列出主要步骤 ​ 1.将/opt/hadoopclient/Spark2x/spark/jars路径下所有的jar包拷贝至/usr/zeppelin/interpreter/spark 1cp &#x2F;opt&#x2F;hadoopclient&#x2F;Spark2x&#x2F;spark&#x2F;jars&#x2F;*.jar &#x2F;usr&#x2F;zeppelin&#x2F;interpreter&#x2F;spark&#x2F; ​ 2.编辑zeppelin-env.sh文件，位置/usr/zeppelin/conf，加入以下内容 1234export JAVA_HOME&#x3D;&#x2F;opt&#x2F;hadoopclient&#x2F;JDK&#x2F;jdk-8u201export MASTER&#x3D;yarn-clientexport SPARK_HOME&#x3D;&#x2F;opt&#x2F;hadoopclient&#x2F;Spark2x&#x2F;sparkexport HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoopclient&#x2F;HDFS&#x2F;hadoop&#x2F;etc&#x2F;hadoop 步骤比较简单，测试提交到yarn集群，却提示(这个问题困扰了我几天。。。) 1234567891011java.lang.ClassCastException: org.apache.hadoop.conf.Configuration cannot be cast to org.apache.hadoop.yarn.conf.YarnConfiguration at org.apache.spark.deploy.yarn.ApplicationMaster.&lt;init&gt;(ApplicationMaster.scala:60) at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$main$1.apply$mcV$sp(ApplicationMaster.scala:679) at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:69) at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:68) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917) at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:68) at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:678) at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala) 下面给出我这边解决方法 适配问题解决网上提示这个报错的解决方法 由于环境中版本不一致导致，需要确保环境中spark版本和hadoop版本为匹配版本： 1.确认spark版本与hadoop版本一致。 2.如果是spark on yarn，同时确认spark配置文件中的spark.yarn.archive或spark.yarn.jar的spark版本。 ​ 我检查了下，zeppelin ${zeppelin_home}/interpreter/spark/ 和 ${zeppelin_home}/lib下的hadoop版本，发现${zeppelin_home}/lib下的hadoop还是2.×的版本(FusionInsight_HD_6.5.1.7 hadoop 3.1.1)。于是满怀期待的替换了，测试[裂开了]还是不好使，依然报同样的错。 ​ 找不到问题只好增加点日志，看看应用读取的spark_defaults.conf，发现了个奇怪的现象，spark.yarn.archive参数有重复。 12spark.yarn.archive value:hdfs://hacluster/user/spark2x/jars/V100R002C80SPC200/spark-archive-2x.zipspark.yarn.archive:hdfs://hacluster/user/spark2x/jars/6.5.1.7/spark-archive-2x.zip 但查看spark_defaults.conf也只发现下面的那一项，V100R002C80SPC200那个没找打在哪配置。。。没办法，修改下zeppelin的源码手动剔除这个配置项，重启问题解决😊。 问题原因：就是spark.yarn.archive配置了不明版本的spark-archive-2x.zip 参考文档Zeppelin连接Spark","categories":[],"tags":[{"name":"spark","slug":"spark","permalink":"https://zhangjunjunah.github.io/tags/spark/"}]},{"title":"在容器中提交yarn-client远端spark任务","slug":"在容器中提交yarn-client远端spark任务","date":"2020-11-09T01:44:17.000Z","updated":"2020-11-10T01:54:27.795Z","comments":true,"path":"2020/11/09/在容器中提交yarn-client远端spark任务/","link":"","permalink":"https://zhangjunjunah.github.io/2020/11/09/%E5%9C%A8%E5%AE%B9%E5%99%A8%E4%B8%AD%E6%8F%90%E4%BA%A4yarn-client%E8%BF%9C%E7%AB%AFspark%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"spark在容器中提交yarn-client任务方案一：容器端口nodePort映射方式方案说明​ 默认情况下， spark-submit 使用pod的 hostname 作为 spark.driver.host ，而 hostname 是pod的主机名，因此 spark executor 无法解析它，而且 spark.driver.port 也在pod（容器）的本地。此方案是通过端口映射方式连接yarn集群。 创建deployement和service的同时，通过nodePort方式暴露spark.driver.port和spark.blockManager.port两个端口 spark-submit提交命令，添加参数 –conf spark.driver.bindAddress=0.0.0.0 #固定值 –conf spark.driver.host=$HOST_IP #k8s节点ip,可以固定为k8s master节点ip（非宿主节点） –conf spark.driver.port=$SPARK_DRIVER_PORT #通过nodePort暴露的端口 –conf spark.driver.blockManager.port=$SPARK_DRIVER_PORT#通过nodePort暴露的端口 方案二：hostNetwork模式方案说明​ 将pod修改为hostNetwork模式，容器可以使用宿主机的网络协议栈，复用宿主机的IP，可以监听端口，Spark也可以回调。 方案三：路由转发方案说明​ 之所以在容器内无法提交spark yarn-client任务，是因为容器内网与hadoop集群不互通，只要想办法解决网络互通，问题自然而然就解决了。下面是我们测试环境的网络转发图。 参考blog在Kubernetes的pod中使用yarn-client调用远端spark集群Spark/k8s：如何用客户机模式在Kubernetes上运行Spark提交k8s骚操作 Pod服务通过IP对外访问&amp;k8s指定IP创建Pod","categories":[],"tags":[{"name":"K8S","slug":"K8S","permalink":"https://zhangjunjunah.github.io/tags/K8S/"}]},{"title":"一次k8s 6443 端口 was refused 解决步骤及思路","slug":"一次k8s-6443-端口-was-refused-解决步骤及思路","date":"2020-10-25T01:19:07.000Z","updated":"2020-10-25T05:00:08.554Z","comments":true,"path":"2020/10/25/一次k8s-6443-端口-was-refused-解决步骤及思路/","link":"","permalink":"https://zhangjunjunah.github.io/2020/10/25/%E4%B8%80%E6%AC%A1k8s-6443-%E7%AB%AF%E5%8F%A3-was-refused-%E8%A7%A3%E5%86%B3%E6%AD%A5%E9%AA%A4%E5%8F%8A%E6%80%9D%E8%B7%AF/","excerpt":"","text":"一、问题现象​ 周一上午同事向我反馈说咱们的测试k8s集群kubectl 命令连接不上提示The connection to the server ip:6443 was refused - did you specify the right host or port?，之前也遇到过这个问题。当时刚刚开始使用k8s，上面还没有pv，为了避免耽误开发卸载重装了(心想这次怎么也得先分析下问题，有点进步才行)，于是慢慢尝试，下面记录了我解决的全过程。 二、解决过程​ 查询了资料 1.检查节点状态1234567891011121314151617181920212223 systemctl status kubelet -l● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Mon 2020-10-12 14:16:53 CST; 26min ago Docs: https://kubernetes.io/docs/ Main PID: 2301 (kubelet) Tasks: 27 Memory: 134.8M CGroup: /system.slice/kubelet.service └─2301 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1Oct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.218811 2301 kubelet.go:2248] node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.318986 2301 kubelet.go:2248] node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.392439 2301 reflector.go:125] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://ip:6443/api/v1/pods?fieldSelector=spec.nodeName%3Ddeeplearn&amp;limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.419206 2301 kubelet.go:2248] node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.478573 2301 handler.go:321] HTTP InternalServerError serving /stats/summary: Internal Error: failed to get node info: node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.478791 2301 handler.go:321] HTTP InternalServerError serving /stats/summary: Internal Error: failed to get node info: node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.481661 2301 handler.go:321] HTTP InternalServerError serving /stats/summary: Internal Error: failed to get node info: node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.519410 2301 kubelet.go:2248] node \"deeplearn\" not foundOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.592251 2301 reflector.go:125] k8s.io/client-go/informers/factory.go:133: Failed to list *v1beta1.RuntimeClass: Get https://ip:6443/apis/node.k8s.io/v1beta1/runtimeclasses?limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 14:43:14 deeplearn kubelet[2301]: E1012 14:43:14.619630 2301 kubelet.go:2248] node \"deeplearn\" not found 没发现什么有价值的信息，于是又查看了系统日志 123456789101112[root@deeplearn log]# tail -f messagesOct 12 10:37:49 deeplearn systemd-logind: New session 1080012 of user root.Oct 12 10:37:49 deeplearn systemd: Started Session 1080012 of user root.Oct 12 10:37:49 deeplearn systemd: Starting Session 1080012 of user root.Oct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.044418 2544 kuberuntime_manager.go:887] PodSandboxStatus of sandbox \"06785948be010cf48cbef180389fd38199517ce1bab59351de39a69ff12b4859\" for pod \"etcd-deeplearn_kube-system(1fdca295cf61791ef3ce86ddb8403ba7)\" error: rpc error: code = Unknown desc = Error response from daemon: open /home/docker/overlay2/2a32175b96d1bd9a6bfc3ff7c484a50c31074f900e7c0fe205c78f1fca8fa417/lower: too many open filesOct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.047370 2544 kuberuntime_manager.go:887] PodSandboxStatus of sandbox \"06785948be010cf48cbef180389fd38199517ce1bab59351de39a69ff12b4859\" for pod \"etcd-deeplearn_kube-system(1fdca295cf61791ef3ce86ddb8403ba7)\" error: rpc error: code = Unknown desc = Error response from daemon: open /home/docker/overlay2/2a32175b96d1bd9a6bfc3ff7c484a50c31074f900e7c0fe205c78f1fca8fa417/lower: too many open filesOct 12 10:37:50 deeplearn systemd-logind: Removed session 1080012.Oct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.148623 2544 reflector.go:125] object-\"kube-system\"/\"gpushare-device-plugin-token-sf97m\": Failed to list *v1.Secret: Get https://ip:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=metadata.name%3Dgpushare-device-plugin-token-sf97m&amp;limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.290671 2544 event.go:249] Unable to write event: 'Patch https://ip:6443/api/v1/namespaces/kube-system/events/etcd-deeplearn.163d0410148c7455: dial tcp ip:6443: connect: connection refused' (may retry after sleeping)Oct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.347797 2544 reflector.go:125] object-\"kube-system\"/\"kube-proxy-token-8fbbs\": Failed to list *v1.Secret: Get https://ip:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=metadata.name%3Dkube-proxy-token-8fbbs&amp;limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 10:37:50 deeplearn kubelet: E1012 10:37:50.547603 2544 reflector.go:125] k8s.io/kubernetes/pkg/kubelet/kubelet.go:444: Failed to list *v1.Service: Get https://ip:6443/api/v1/services?limit=500&amp;resourceVersion=0: dial tcp ip:6443: connect: connection refusedOct 12 10:37:50 deeplearn kubelet: W1012 10:37:50.747777 2544 status_manager.go:485] Failed to get status for pod \"kube-controller-manager-deeplearn_kube-system(ba945724332cd7f7efca6f11bdcf8307)\": Get https://ip:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-deeplearn: dial tcp ip:6443: connect: connection refused 发现了可疑日志too many open files too many open files是Linux系统中常见的错误，从字面意思上看就是说程序打开的文件数过多，不过这里的files不单是文件的意思，也包括打开的通讯链接(比如socket)，正在监听的端口等等，所以有时候也可以叫做句柄(handle)，这个错误通常也可以叫做句柄数超出系统限制。 是哪个进程占用了过多的句柄？ 发现docker 和k8s 占据前两名(8225是一个业务进程) 1234[root@deeplearn ~]# lsof -n|awk '&#123;print $2&#125;'|sort|uniq -c|sort -nr|more5901300 27593 20532 1081 17152 8225 2.清理系统句柄​ 尝试使用docker system df清理也提示error reading dir entries: open /run/docker/plugins: too many open files（使用ulimit -a查询open files参数已调至系统上限），没办法只有先停止后面的业务进程了。 ​ 杀掉业务系统，执行docker 清理命令，文件句柄数下降但k8s依然没有恢复正常。。。 123456docker system prune docker image prunedocker container prunedocker volume prune docker network prunedocker rmi $(docker images -q) 3.导入基础镜像​ 到这里问题线索又断了，没办法再上网查查看看有没有类似的问题现象吧。在一篇blog中发现了类似的日志《问题“The connection to the server….:6443 was refused - did you specify the right host or port?”的处理！》。 按照blog中的解决方法–重新导入基础镜像,问题解决！ 三、问题总结 ​ 基础镜像无故消失，导致问题产生。 ​ 系统文件句柄数过高,导致系统服务异常。","categories":[],"tags":[{"name":"K8S","slug":"K8S","permalink":"https://zhangjunjunah.github.io/tags/K8S/"}]},{"title":"Java操作docker步骤及问题总结","slug":"Java操作docker步骤及问题总结","date":"2020-07-18T05:33:06.000Z","updated":"2020-07-18T07:47:00.223Z","comments":true,"path":"2020/07/18/Java操作docker步骤及问题总结/","link":"","permalink":"https://zhangjunjunah.github.io/2020/07/18/Java%E6%93%8D%E4%BD%9Cdocker%E6%AD%A5%E9%AA%A4%E5%8F%8A%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","excerpt":"","text":"一、前言​ 这段时间一直在使用docker和kubernetes，最近产品有个需求在业务上迭代镜像，需要通过Java 远程操作docker。我想这种方案网上应该挺多的，很容易完成，但工作不是像我想的那样顺利。。。下面附上操作步骤及遇到的一些问题和我的解决思路。 二、操作步骤(参考网上)​ 此部分主要参考网上的一些blog 配置Docker两台docker主机(linux Centos 7)，其中一台使用GPU(不是我负责的，机器环境不太熟，这里留了一个坑) 1.修改docker.service，ExecStart=/usr/bin/dockerd 添加上 -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 12345678vi /lib/systemd/system/docker.service[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by docker#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 2.修改后保存文件，然后通知docker服务做出的修改 12systemctl daemon-reloadservice docker restart 3.测试使用docker命令远程方式访问 1docker -H tcp://ip:2375 images 配置产品使用docker-javadocker-java推荐使用maven项目引入相关的jar包依赖： 123456&lt;dependency&gt; &lt;groupId&gt;com.github.docker-java&lt;/groupId&gt; &lt;artifactId&gt;docker-java&lt;/artifactId&gt; &lt;!-- use latest version https://github.com/docker-java/docker-java/releases --&gt; &lt;version&gt;3.X.Y&lt;/version&gt;&lt;/dependency&gt; 三、问题解决方法上面的步骤配置不复杂，但实际操作后，还是出现了些问题，在这里记录一下。 1.在/lib/systemd/system/docker.service配置了-H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock，但是配置无效 解决方法: 进入/etc/systemd/system/docker.service.d目录，检查override.conf文件是否覆盖了[Service] ExecStart的配置 2.产品配置了docker-java maven依赖项目连接失败并提示报错: 123456789Exception in thread \"main\" java.lang.NoSuchMethodError: javax.ws.rs.core.MultivaluedMap.addAll(Ljava/lang/Object;[Ljava/lang/Object;)V at org.glassfish.jersey.client.ClientRequest.accept(ClientRequest.java:336) at org.glassfish.jersey.client.JerseyInvocation$Builder.accept(JerseyInvocation.java:240) at org.glassfish.jersey.client.JerseyInvocation$Builder.accept(JerseyInvocation.java:163) at com.github.dockerjava.jaxrs.CommitCmdExec.execute(CommitCmdExec.java:35) at com.github.dockerjava.jaxrs.CommitCmdExec.execute(CommitCmdExec.java:15) at com.github.dockerjava.jaxrs.AbstrSyncDockerCmdExec.exec(AbstrSyncDockerCmdExec.java:23) at com.github.dockerjava.core.command.AbstrDockerCmd.exec(AbstrDockerCmd.java:35) at com.github.dockerjava.core.command.CommitCmdImpl.exec(CommitCmdImpl.java:347) 解决方法: ​ 此问题的原因是jar包版本冲突，检查项目中是否使用了hadoop相关jar包，去除jersey-core和jersey-server相关依赖 12345678910111213141516171819202122232425262728293031323334&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;$&#123;cdh.hadoop.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;jersey-json&lt;/artifactId&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-server&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;cdh.hadoop.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-server&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 四、解决思路​ 第三部分是我操作过程中遇到的两个问题的解决方法，解决方法看上去也挺简单的，但解决的过程也比较的心酸(水平有待提高)，特意记录下。 ​ 1.GPU主机配置了Docker远程访问，但配置无效。。。 当时第一反应是看看有没有其他的配置方法，可能GPU主机配置了nvidia的相关信息，换种方法试试。修改/etc/docker/daemon.json，添加键值对 &quot;hosts&quot;: [&quot;0.0.0.0:2375&quot;,&quot;unix:///var/run/docker.sock&quot;]，但事与愿违，这种方式docker压根启动不了。在添加这段内容时，也发现了现象，即使我把deamon.json里的配置都删了，通过systemctl status docker还是能够看到dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime。 由于当时还有其他任务，就向当时安装这台GPU主机docker的同事请求协助，可能有啥特殊配置我不知道。过了两天，我这边紧急的事忙完了，回过来解决遗留的问题，自己的事要负责到底啊(下面重点来了)。 ​ 我自己还是坚持认为是有啥特殊配置导致这里不生效，二话不说我全局搜一下docker相关的配置文件不就行了，find . -name *docker*,依次检查了相关的配置没发现什么问题。上网搜了下docker有哪些配置文件，依然没什么收获。这里有点头大，于是我换了一种思路检查问题，我不是配置了两台主机了，那台CPU机器可以正常使用于是我分别用docker info和systemctl status docker命令查看两台主机有什么区别，终于在执行了systemctl status docker发现了些区别。 1234567891011121314151617181920212223242526272829 #GPU机器 systemctl status docker -l● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/docker.service.d └─override.conf Active: active (running) since Sat 2020-07-18 11:37:53 CST; 1h 39min ago Docs: https://docs.docker.com Main PID: 19626 (dockerd) Tasks: 91 Memory: 183.1M CGroup: /system.slice/docker.service ├─19626 /usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime ├─19865 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 20000 -container-ip 192.167.0.3 -container-port 20000 ├─21907 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9380 -container-ip 192.167.0.2 -container-port 9380 └─21925 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9360 -container-ip 192.167.0.2 -container-port 9360 #CPU机器 systemctl status docker -l● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2020-07-14 16:09:34 CST; 3 days ago Docs: https://docs.docker.com Main PID: 17952 (dockerd) Tasks: 92 Memory: 1.4G CGroup: /system.slice/docker.service ├─17952 /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock └─18512 /usr/bin/docker-proxy -proto tcp -host-ip 127.0.0.1 -host-port 1514 -container-ip 172.18.0.7 -container-port 10514 GPU机器多了一段Drop-In,查看override.conf,发现了问题所在，这里覆写了ExecStart相关配置，至此这个问题终于解决了。 12Drop-In: /etc/systemd/system/docker.service.d └─override.conf","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zhangjunjunah.github.io/tags/Docker/"},{"name":"Java","slug":"Java","permalink":"https://zhangjunjunah.github.io/tags/Java/"}]},{"title":"flannel组件挂了导致k8s网络通信失败","slug":"flannel组件挂了导致k8s网络通信失败","date":"2020-06-09T02:34:53.633Z","updated":"2020-06-09T02:34:53.633Z","comments":true,"path":"2020/06/09/flannel组件挂了导致k8s网络通信失败/","link":"","permalink":"https://zhangjunjunah.github.io/2020/06/09/flannel%E7%BB%84%E4%BB%B6%E6%8C%82%E4%BA%86%E5%AF%BC%E8%87%B4k8s%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%A4%B1%E8%B4%A5/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"docker 镜像瘦身(commit 方式制作的镜像)","slug":"docker 镜像瘦身(commit 方式制作的镜像)","date":"2020-04-22T16:00:00.000Z","updated":"2020-04-23T06:20:04.649Z","comments":true,"path":"2020/04/23/docker 镜像瘦身(commit 方式制作的镜像)/","link":"","permalink":"https://zhangjunjunah.github.io/2020/04/23/docker%20%E9%95%9C%E5%83%8F%E7%98%A6%E8%BA%AB(commit%20%E6%96%B9%E5%BC%8F%E5%88%B6%E4%BD%9C%E7%9A%84%E9%95%9C%E5%83%8F)/","excerpt":"","text":"背景​ 根据产品的运行作业的特点，我们需要制作一些业务镜像，将作业放到容器中执行。由于制作运行环境要不断调试，我们是通过大致(有省略)以下步骤制作的: 启动一个空容器(ubuntu) 安装python/pip 安装类库 将作业放到容器中测试;如果测试通过，docker commit 方式提交镜像 ​ 在这里要反思下，当时由于没有充分调研docker镜像正确的制作方式，导致后期迭代镜像越来越大(10G+)，以及想重新制作却没有了前面版本的制作轨迹。 ​ 近期，由于产品要在一个私有环境部署，镜像太大传输太慢，所以有了镜像瘦身的想法。 步骤 运行容器并进入，查看磁盘占用，删除不需要的数据 12345678#运行容器并进入容器样例docker run -d --name ubuntu ubuntu:15.10docker exec -i -t ubuntu /bin/bash#进入根目录cd / #查看各个目录体积du -h -d 1#删除不需要的数据(略) 在容器根目录打包容器数据(剔除/proc、/sys目录) 1234#通过tar方式打包tar --exclude=proc --exclude=sys --exclude=base_img.tar -cvf images.tar .#完成后退出容器exit 将容器内的tar包拷贝出来，再将镜像重新导入 1234#拷贝容器中的文件到宿主机中docker cp ubuntu:/images.tar .#将镜像重新导入cat images.tar|docker import - new-ubuntu 完成瘦身，比较瘦身前后的体积 1docker images |grep new-ubuntu 参考博文docker容器commit的镜像越来越大怎么办？酱紫试试","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zhangjunjunah.github.io/tags/Docker/"}]},{"title":"Log4J2.xml自用配置","slug":"Log4J2.xml自用配置","date":"2020-04-20T13:53:40.000Z","updated":"2020-04-21T03:07:34.360Z","comments":true,"path":"2020/04/20/Log4J2.xml自用配置/","link":"","permalink":"https://zhangjunjunah.github.io/2020/04/20/Log4J2.xml%E8%87%AA%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"","text":"前言​ log4j2的配置文件网上一搜很容易搜到，这里主要就是记录下自己开发中经常使用的一套配置，方便以后查找。 配置文件详情123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!--Configuration后面的status，这个用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，你会看到log4j2内部各种详细输出--&gt;&lt;!--monitorInterval：Log4j能够自动检测修改配置 文件和重新配置本身，设置间隔秒数--&gt;&lt;configuration status=\"error\" monitorInterval=\"30\"&gt; &lt;!--全局参数--&gt; &lt;Properties&gt; &lt;Property name=\"pattern\"&gt;[%style&#123;%d&#125;&#123;bright,green&#125;][%highlight&#123;%p&#125;][%style&#123;%t&#125;&#123;bright,blue&#125;][%style&#123;%C.%M:%L&#125;&#123;bright,yellow&#125;]: %msg%n%style&#123;%throwable&#125;&#123;red&#125;&lt;/Property&gt; &lt;Property name=\"logDir\"&gt;$&#123;env:IM_HOME&#125;/logs&lt;/Property&gt; &lt;Property name=\"disableAnsi\"&gt;false&lt;/Property&gt; &lt;/Properties&gt; &lt;Loggers&gt; &lt;Root level=\"INFO\"&gt; &lt;AppenderRef ref=\"console\"/&gt; &lt;AppenderRef ref=\"rolling_file\"/&gt; &lt;/Root&gt; &lt;/Loggers&gt; &lt;Appenders&gt; &lt;!-- 定义输出到控制台 --&gt; &lt;Console name=\"console\" target=\"SYSTEM_OUT\" follow=\"true\"&gt; &lt;!--控制台只输出level及以上级别的信息--&gt; &lt;ThresholdFilter level=\"DEBUG\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;PatternLayout &gt; &lt;Pattern&gt;$&#123;pattern&#125;&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;/Console&gt; &lt;!-- 同一来源的Appender可以定义多个RollingFile，定义按天存储日志 --&gt; &lt;RollingFile name=\"rolling_file\" fileName=\"$&#123;logDir&#125;/im-server.log\" filePattern=\"$&#123;logDir&#125;/im-server_%d&#123;yyyy-MM-dd&#125;.log\"&gt; &lt;ThresholdFilter level=\"INFO\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;$&#123;pattern&#125;&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy interval=\"1\"/&gt; &lt;/Policies&gt; &lt;!-- 日志保留策略，配置只保留七天 --&gt; &lt;DefaultRolloverStrategy&gt; &lt;Delete basePath=\"$&#123;logDir&#125;/\" maxDepth=\"1\"&gt; &lt;IfFileName glob=\"im-server_*.log\" /&gt; &lt;IfLastModified age=\"7d\" /&gt; &lt;/Delete&gt; &lt;/DefaultRolloverStrategy&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt;&lt;/configuration&gt; 说明 控制台日志高亮设置(IDE为IDEA),在启动主类VM options 中添加一行参数:-Dlog4j.skipJansi=false 在logDir属性配置中可以通过启动类的环境变量(Environment variables)设置IM_HOME路径","categories":[],"tags":[{"name":"Log4J2","slug":"Log4J2","permalink":"https://zhangjunjunah.github.io/tags/Log4J2/"}]},{"title":"Spring AOP结合自定义注解实例","slug":"spring AOP结合自定义注解实例","date":"2020-03-30T16:00:00.000Z","updated":"2020-04-21T02:57:39.846Z","comments":true,"path":"2020/03/31/spring AOP结合自定义注解实例/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/31/spring%20AOP%E7%BB%93%E5%90%88%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E5%AE%9E%E4%BE%8B/","excerpt":"","text":"使用背景​ 近期同事使用产品经常遇到页面点击卡死长时间未响应现象(本人比较擅长解决问题)，心里带着疑问使用jstack命令查看jvm堆栈，发现阻塞的线程都是表更新操作(update)。查看了下代码发现是由于最近大家申请容器资源频繁，申请容器逻辑在一个事务中，又因为资源告警创建时间变长，导致锁表。在这里简单说下申请容器的步骤。 完成资源申请合法校验，更新资源表信息 通过k8sApi创建需要的资源(pod、deployment、job、…) –在资源紧张时创建时间较长 创建失败删除对应的k8s资源 备注：资源表记录了容器平台资源总共多少资源（CPU、GPU、memory），已使用多少等信息 逻辑很简单，但创建容器频繁容易造成锁表，导致页面卡死。改造方案也很简单，更新资源表不走事务，如果创建逻辑失败，手动回滚，不就可以了。代理设计模式最适合这种改造场景，话不多说赶紧动手。 使用实例创建注解123456789101112/*** @Description: 申请容器(服务)资源注解* @Param:* @return:* @Author: zhangjj* @Date: 2020-02-25*/@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface ApplyContainerRes &#123;&#125; 编写切面在这里只放上了相关的代码（包名涉及公司暂时屏蔽了） 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @Description: 申请资源切面 * @ClassName: ApplyContainerResAspect * @Author: zhangjj * @Date: 2020-02-25 */@Aspect@Component@Slf4jpublic class ApplyContainerResAspect &#123; @Autowired private DataSourceTransactionManager dataSourceTransactionManager; @Pointcut(\"@annotation(xx.ApplyContainerRes)\") public void annotationApplyContainerResPointCut()&#123; &#125; @Before(\"annotationApplyContainerResPointCut()\") public void doBefore(JoinPoint joinPoint) &#123; DefaultTransactionDefinition def = new DefaultTransactionDefinition(); //新发起一个事务 def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW); TransactionStatus transaction = dataSourceTransactionManager.getTransaction(def); //伪代码 //申请资源(更新资源表信息) applyRes(joinPoint); dataSourceTransactionManager.commit(transaction); &#125; @AfterThrowing(\"annotationApplyContainerResPointCut()\") public void doAfter(JoinPoint joinPoint) &#123; DefaultTransactionDefinition def = new DefaultTransactionDefinition(); //新发起一个事务 def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW); TransactionStatus transactionStatus = dataSourceTransactionManager.getTransaction(def); //伪代码 //释放资源(更新资源表信息) releaseResource(joinPoint); dataSourceTransactionManager.commit(transactionStatus); &#125;&#125; 业务使用service层业务逻辑样例 12345678910111213141516/*** @Description: 启动服务* @Param: [environmentId,modelServices]* @return: OperContainerResult* @Author: zhangjj* @Date: 2019-08-07*/@Override@ApplyContainerRespublic OperContainerResult startServing(Long environmentId, ModelServices modelServices) &#123; OperContainerResult operContainerResult = new OperContainerResult(); //服务部署 deployModelService(modelServices); operContainerResult.setCode(OperContainerResult.CODE.SUCCESS); return operContainerResult;&#125; 改造完成，测试通过！很顺利嘛(手动怀疑)，可以让同事仿照上面样例改造其他业务逻辑了。 使用中的问题​ 同事的改造速度也很快，本地测试发现不对，资源表的数据怎么没更新啊！我这边心想咋回事，我这边使用都好使，怎么你这却不好使了。。。一起查看逻辑，发现他那的逻辑是在同一个类中调用。我这边意识到动态代理好像不能代理类中方法的直接调用，网上搜搜看看有没有解决方法,很快找到了解决办法。 未调整前代码 1234567891011121314151617181920/** * @param batchWorkId * @Description: 批量作业运行 * @Param: [batchWorkId] * @return: void * @Author: zhangjj * @Date: 2020-01-03 */@Overridepublic void runBatchWork(Long batchWorkId) &#123; //其他业务 dosomething(); //调用被代理的代理类 doRunBatchWork(batchWorkId);&#125;@ApplyContainerRespublic void doRunBatchWork(batchWorkId) &#123; //业务逻辑&#125; 网上的解决方案 既然 doRunBatchWork() 方法调用没有触发 AOP 逻辑的原因是因为我们以目标对象的身份(target object) 来调用的, 那么解决的关键自然就是以代理对象(proxied object)的身份来调用 doRunBatchWork() 方法。 备注:由于这里的调用方法没有声明在接口中，需要将jdk动态代理调整为cglib动态代理(这里先留个坑，以后可以总结下两者的区别) 调整后的代码 12345678910111213141516171819202122232425262728//在springboot启动类上添加下面一行注解@EnableAspectJAutoProxy(exposeProxy=true,proxyTargetClass=true)//业务伪代码@Autowiredprivate XXServiceImpl self;/** * @param batchWorkId * @Description: 批量作业运行 * @Param: [batchWorkId] * @return: void * @Author: zhangjj * @Date: 2020-01-03 */@Overridepublic void runBatchWork(Long batchWorkId) &#123; //其他业务 dosomething(); //调用被代理的代理类 self.doRunBatchWork(batchWorkId);&#125;@ApplyContainerRespublic void doRunBatchWork(batchWorkId) &#123; //业务逻辑&#125; 修改后，代理逻辑生效。不过，没过多久同事又反馈另外一个逻辑这么改造还是不好使，我查看了下发现方法声明是private(被代理的类必须要是public声明)。之前没意识到spring AOP使用中会遇到这些问题，在这里也简单总结下，方便查看回忆。 使用总结 springboot使用cglib代码，在启动类上添加以下注解@EnableAspectJAutoProxy(exposeProxy=true,proxyTargetClass=true) 被代理的方法尽量跨类调用 被代理的方法不能声明为私有方法","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://zhangjunjunah.github.io/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://zhangjunjunah.github.io/tags/Spring/"}]},{"title":"记录一次因内存原因导致k8s创建pod失败问题","slug":"记录一次因内存原因导致k8s创建pod失败问题","date":"2020-03-23T16:00:00.000Z","updated":"2020-04-21T02:57:39.847Z","comments":true,"path":"2020/03/24/记录一次因内存原因导致k8s创建pod失败问题/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/24/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E5%9B%A0%E5%86%85%E5%AD%98%E5%8E%9F%E5%9B%A0%E5%AF%BC%E8%87%B4k8s%E5%88%9B%E5%BB%BApod%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题背景​ 最近同事反馈使用k8s创建jupyter经常在某一台节点创建不起来(其他节点正常)，如果把这台节点的docker重启，当时可以恢复正常，但过段时间问题又会复现,下面附上kubectl describe部分日志。 12345Events: Type Reason Age From Message---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 1s (x13 over 18s) kubelet, centos7-141 Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \"jupyter-lab1584951113027\": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:303: getting the final child's pid from pipe caused \\\"EOF\\\"\": unknown 问题分析​ 带着疑问上网查询报错，有的说是docker版本、centos 内核与k8s版本不兼容导致？ ​ 依次查询了几台主机的docker版本(k8s是一起安装的所以直接排除)，都是19.03.5版本(centos版本也一致)。但仔细想想其他节点都没出现这个问题，可能问题原因不在这，暂时排除。 123456789101112131415161718192021222324252627Client: Docker Engine - Community Version: 19.03.5 API version: 1.40 Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:25:41 2019 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 19.03.5 API version: 1.40 (minimum version 1.12) Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:24:18 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.6 GitCommit: 894b81a4b802e4eb2a91d1ce216b8817763c29fb runc: Version: 1.0.0-rc8 GitCommit: 425e105d5a03fabd737a126ad93d62a9eeede87f docker-init: Version: 0.18.0 GitCommit: fec3683 ​ 问题困扰了一两天，后来看到一篇文章Kubernetes因限制内存配置引发的错误给我启发，会不会是内存原因导致的问题。使用free 命令对比了两台节点的内存。 1234567891011# 问题节点[root@centos7-141 log]# free -g total used free shared buff/cache availableMem: 22 1 0 1 20 12Swap: 0 0 0# 正常节点[root@centos7-142 ~]# free -g total used free shared buff/cache availableMem: 22 2 9 1 10 15Swap: 0 0 0 等等，发现了一些端倪，为啥总计22g，只使用1g,剩余内存为啥是0？还有buff/cache 是啥？带着疑问，查询了 buff/cache,在这记录下。 Linux服务器运行一段时间后，由于其内存管理机制，会将暂时不用的内存转为buff/cache，这样在程序使用到这一部分数据时，能够很快的取出，从而提高系统的运行效率，所以这也正是linux内存管理中非常出色的一点，所以乍一看内存剩余的非常少，但是在程序真正需要内存空间时，linux会将缓存让出给程序使用，这样达到对内存的最充分利用，所以真正剩余的内存是free+buff/cache 在这里按照先解决的原则，按照操作步骤释放了buff/cache。 123456echo 1 &gt; /proc/sys/vm/drop_caches#操作后的内存占用[root@centos7-141 log]# free -g total used free shared buff/cache availableMem: 22 1 7 1 13 12Swap: 0 0 重新创建pod，创建成功，问题解决！ 1234567891011Name: jupyter-lab1584951113027Namespace: kubeflowPriority: 0Node: centos7-141/192.168.128.141Start Time: Tue, 24 Mar 2020 14:00:46 +0800Labels: app=jupyterhub component=singleuser-server heritage=jupyterhubAnnotations: hub.jupyter.org/username: lab1584951113027Status: RunningIP: 10.244.3.3 后记​ 尽管问题解决了，但还是有两个疑问，在这里先记录下来 buff/cache为啥没有在k8s需要内存时及时让出？ 如果buff/cache没有及时让出缓存，为啥k8s events 不提示内存超出限制报错？ 参考 Kubernetes因限制内存配置引发的错误 Linux释放内存空间","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zhangjunjunah.github.io/tags/Docker/"},{"name":"K8S","slug":"K8S","permalink":"https://zhangjunjunah.github.io/tags/K8S/"}]},{"title":"Ceph 配置RGW高可用","slug":"ceph 配置 RGW高可用","date":"2020-03-20T16:00:00.000Z","updated":"2020-04-21T02:57:39.844Z","comments":true,"path":"2020/03/21/ceph 配置 RGW高可用/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/21/ceph%20%E9%85%8D%E7%BD%AE%20RGW%E9%AB%98%E5%8F%AF%E7%94%A8/","excerpt":"","text":"ceph节点规划 节点ip 节点主机名 节点ceph组件 其他相关组件 192.168.153.51 centos 7-ceph-1（ceph 主节点） mds1、mon1、mg1、rgw1、osd1 keepalived 192.168.153.52 centos 7-ceph-2 mon2、mg2、rgw2、osd2 keepalived 192.168.153.53 centos 7-ceph-3 mon3、mg3、osd3 整体架构 新增RGW服务 1234567891011#在centos 7-ceph-1节点使用cephuser用户cd my-clusterceph-deploy rgw create 128# 访问测试curl -I http://centos 7-ceph-1:7480/HTTP/1.1 200 OKx-amz-request-id: tx000000000000000000001-005d0b0e2a-1018-defaultContent-Type: application/xmlContent-Length: 0Date: Thu, 20 Jun 2019 04:40:11 GMT 安装相关软件安装依赖1234#在centos 7-ceph-1、centos 7-ceph-2节点使用root用户执行yum -y install openssl-devel --skip-brokenyum install -y libnl3-devel libnfnetlink-develyum -y install gcc 安装keepalived 12345678910111213141516171819#在centos 7-ceph-1、centos 7-ceph-2节点使用root用户执行cd /usr/local/src#官网下载keepalived的最新版本，解压并安装wget http://www.keepalived.org/software/keepalived-2.0.7.tar.gztar xvf keepalived-2.0.7.tar.gzcd keepalived-2.0.7./configure --prefix=/usr/local/keepalivedmake &amp;&amp; make install#初始化及启动#keepalived启动脚本变量引用文件，默认文件路径是/etc/sysconfig/，也可以不做软链接，直接修改启动脚本中文件路径即可（安装目录下）cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/keepalived # 将keepalived主程序加入到环境变量（安装目录下）cp /usr/local/keepalived/sbin/keepalived /usr/sbin/keepalived# keepalived启动脚本（源码目录下），放到/etc/init.d/目录下就可以使用service命令便捷调用cp /usr/local/src/keepalived-2.0.7/keepalived/etc/init.d/keepalived /etc/init.d/keepalived # 将配置文件放到默认路径下mkdir /etc/keepalivedcp /usr/local/keepalived/etc/keepalived/keepalived.conf etc/keepalived/keepalived.conf keepalived.conf配置12345678910111213141516171819202122232425262728293031#centos 7-ceph-1节点cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bakcat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123;&#125;vrrp_script chk_rgw &#123; script \"/usr/local/keepalived/sbin/check_rgw.sh\" # 该脚本检测rgw的运行状态，并在rgw进程挂了之后尝试重新启动rgw，如果启动失败则停止keepalived，准备让其它机器接管。 interval 2 # 每2s检测一次 weight 2 # 检测失败（脚本返回非0）则优先级2&#125;vrrp_instance VI_1 &#123; state MASTER # 指定keepalived的角色，MASTER表示此主机是主服务器，BACKUP表示此主机是备用服务器 interface ens32 # 指定HA监测网络的接口 根据你实际的网卡名来 keyong ip addr 查询 virtual_router_id 51 # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。即同一vrrp_instance下，MASTER和BACKUP必须是一致的 priority 100 # 定义优先级，数字越大，优先级越高，在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级 advert_int 1 # 设定MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS # 设置验证类型，主要有PASS和AH两种 auth_pass 1111 # 设置验证密码，在同一个vrrp_instance下，MASTER与BACKUP必须使用相同的密码才能正常通信 &#125; virtual_ipaddress &#123; 192.168.153.16 # 设置虚拟IP地址(与节点ip同网段) &#125; track_script &#123; chk_rgw # 引用VRRP脚本，即在 vrrp_script 部分指定的名字。定期运行它们来改变优先级，并最终引发主备切换。 &#125;&#125; 12345678910111213141516171819202122232425262728293031#centos 7-ceph-2节点cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bakcat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123;&#125;vrrp_script chk_rgw &#123; script \"/usr/local/keepalived/sbin/check_rgw.sh\" # 该脚本检测rgw的运行状态，并在rgw进程挂了之后尝试重新启动rgw，如果启动失败则停止keepalived，准备让其它机器接管。 interval 2 # 每2s检测一次 weight 2 # 检测失败（脚本返回非0）则优先级2&#125;vrrp_instance VI_1 &#123; state BACKUP # 指定keepalived的角色，MASTER表示此主机是主服务器，BACKUP表示此主机是备用服务器 interface ens32 # 指定HA监测网络的接口 根据你实际的网卡名来 keyong ip addr 查询 virtual_router_id 51 # 虚拟路由标识，这个标识是一个数字，同一个vrrp实例使用唯一的标识。即同一vrrp_instance下，MASTER和BACKUP必须是一致的 priority 100 # 定义优先级，数字越大，优先级越高，在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级 advert_int 1 # 设定MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS # 设置验证类型，主要有PASS和AH两种 auth_pass 1111 # 设置验证密码，在同一个vrrp_instance下，MASTER与BACKUP必须使用相同的密码才能正常通信 &#125; virtual_ipaddress &#123; 192.168.153.16 # 设置虚拟IP地址(与节点ip同网段) &#125; track_script &#123; chk_rgw # 引用VRRP脚本，即在 vrrp_script 部分指定的名字。定期运行它们来改变优先级，并最终引发主备切换。 &#125;&#125; centos 7-ceph-1/2 中，/usr/local/keepalived/sbin/check_rgw.sh脚本内容如下 1234567891011#!/bin/bashif [ \"$(ps -ef | grep \"radosgw\"| grep -v grep )\" == \"\" ];then systemctl start ceph-radosgw.target sleep 3 if [ \"$(ps -ef | grep \"radosgw\"| grep -v grep )\" == \"\" ];then systemctl stop keepalived fifi#添加check_rgw.sh脚本执行权限chmod +x /usr/local/keepalived/sbin/check_rgw.sh 到这里对keepalived的配置已经完成，然后分别启动centos 7-ceph-1/2点上的keepalived 1systemctl start keepalived 分别在centos 7-ceph-1和centos 7-ceph-2节点上执行ip a命令，查看虚IP信息： 1234567891011121314151617181920212223242526272829#centos 7-ceph-11: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:67:df:45 brd ff:ff:ff:ff:ff:ff inet 192.168.153.51/24 brd 192.168.129.255 scope global noprefixroute ens32 valid_lft forever preferred_lft forever inet 192.168.153.16/32 scope global ens32 valid_lft forever preferred_lft forever inet6 fe80::388c:a9c2:c50b:dd48/64 scope link noprefixroute valid_lft forever preferred_lft forever#centos 7-ceph-21: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:4c:7f:2a brd ff:ff:ff:ff:ff:ff inet 192.168.153.52/24 brd 192.168.129.255 scope global noprefixroute ens32 valid_lft forever preferred_lft forever inet6 fe80::58cc:e799:bc61:7fd5/64 scope link noprefixroute valid_lft forever preferred_lft forever 调整S3配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[cephuser@centos 7-ceph-1 keepalived-2.0.7]$ s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key [AY96HNDL9H2QV2SRPNDJ]: Secret Key [Q8OhUOJdRJg8KzAUwZ22BddL7QbC6g8hHCGwq1jx]: Default Region [US]: Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.S3 Endpoint [192.168.153.51:7480]: 192.168.153.16:7480Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [192.168.153.51:7480]: 192.168.153.16:7480Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: Path to GPG program [/bin/gpg]: When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [No]: On some networks all internet access must go through a HTTP proxy.Try setting it here if you can't connect to S3 directlyHTTP Proxy server name: New settings: Access Key: AY96HNDL9H2QV2SRPNDJ Secret Key: Q8OhUOJdRJg8KzAUwZ22BddL7QbC6g8hHCGwq1jx Default Region: US S3 Endpoint: 192.168.153.16:7480 DNS-style bucket+hostname:port template for accessing a bucket: 192.168.153.16:7480 Encryption password: Path to GPG program: /bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0Test access with supplied credentials? [Y/n] yPlease wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] yConfiguration saved to '/home/cephuser/.s3cfg'[cephuser@centos 7-ceph-1 keepalived-2.0.7]$ s3cmd ls2020-03-11 07:16 s3://MyBucket_12020-03-11 07:45 s3://algorithm-bucket2020-03-11 07:44 s3://demo-bucket 参考引用keepalived配置RGW高可用","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zhangjunjunah.github.io/tags/Linux/"},{"name":"Ceph","slug":"Ceph","permalink":"https://zhangjunjunah.github.io/tags/Ceph/"}]},{"title":"Ceph和s3的安装","slug":"ceph和s3的安装","date":"2020-03-20T16:00:00.000Z","updated":"2020-04-21T02:57:39.845Z","comments":true,"path":"2020/03/21/ceph和s3的安装/","link":"","permalink":"https://zhangjunjunah.github.io/2020/03/21/ceph%E5%92%8Cs3%E7%9A%84%E5%AE%89%E8%A3%85/","excerpt":"","text":"部署说明 安装的ceph版本（14.2.4）、s3 版本(2.0.2) ceph使用ceph-deploy部署高可用集群至少需要3台节点(ceph 监视器需要3个以上节点) ceph推荐使用裸盘安装osd(文件夹也可以部署，但不推荐) ceph部署节点规划 节点ip 节点主机名 节点ceph组件 192.168.153.51 centos 7-ceph-1（ceph 主节点） mds1、mon1、mg1、rgw、osd1 192.168.153.52 centos 7-ceph-2 mon2、mg2、osd2、 192.168.153.53 centos 7-ceph-3 mon3、mg3、osd3 创建部署用户注意，在外网环境，禁止使用 ceph 服务名 作为用户，防止暴力破解。 因为 ceph-deploy 部署工具需要以登录用户（且包含sudo权限）来安装软件和做配置，因此通常在主节点创建 ceph 用户。 12345678910111213141516171819# 在每一个节点执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）useradd cephuserecho 'cephuser' | passwd --stdin cephuserecho \"cephuser ALL = (root) NOPASSWD:ALL\" &gt; /etc/sudoers.d/cephuserchmod 0440 /etc/sudoers.d/cephuser# 配置sshd可以使用password登录sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_configsystemctl reload sshd# 配置sudo不需要ttysed -i 's/Default requiretty/#Default requiretty/' /etc/sudoers#每个节点添加hosts192.168.153.51 centos 7-ceph-1192.168.153.52 centos 7-ceph-2192.168.153.53 centos 7-ceph-3# 设置免密登录（使用cephuser用户）ssh-keygen -t rsa # 在centos 7-ceph-1使用cephuser用户操作ssh-copy-id -i /home/cephuser/.ssh/id_rsa.pub cephuser@centos 7-ceph-2ssh-copy-id -i /home/cephuser/.ssh/id_rsa.pub cephuser@centos 7-ceph-3 创建虚拟卷通过虚拟机管理界面分别为每个节点 挂载新磁盘 12345# 每台节点操作(root)fdisk -l #查看挂载的磁盘名，#格式化磁盘parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%mkfs.xfs /dev/sdb -f 关闭 SetLinux（建议） 12345# 建议执行，注意需要重启机器setenforce 0 sed -i \"s/SELINUX=enforcing/SELINUX=disabled /g\" /etc/selinux/config # 重启机器，检查，如果SELinux status参数为disabled即为关闭状态/usr/sbin/sestatus -v 关闭防火墙 123# 可选，如果不关闭需要开启对应端口，本例简单的关闭处理systemctl disable firewalld.service systemctl stop firewalld.service 配置ntp服务器 12345678910111213141516# 在三台节点root执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）yum -y install ntp# 将centos 7-ceph-1配置成ntp服务端[root@centos7-ceph-1 ~]# cat /etc/ntp.confserver 127.127.1.0fudge 127.127.1.0 stratum 10restrict 192.168.153.1 mask 255.255.255.0 nomodify notrap#启动ntpsystemctl start ntpd &amp;&amp; systemctl enable ntpd# centos 7-ceph-2、centos 7-ceph-3配置ntp客户端cat /etc/ntp.conf server 192.168.153.51#执行手动同步ntpdate 192.168.153.51systemctl start ntpd &amp;&amp; systemctl enable ntpd 开始部署安装 ceph 依赖 12345# ceph-deploy 是ceph 的部署工具，在管理节点(centos 7-ceph-1执行)yum install -y ceph-deploy ceph-deploy --version // 2.0.1# 安装 ceph 和 radosgw ,在每台节点执行（centos 7-ceph-1、centos 7-ceph-2、centos 7-ceph-3）yum install -y ceph ceph-radosgw 搭建 ceph 服务 12345678910111213141516171819202122232425262728293031323334# 注意以下所有使用 ceph 用户执行，不要使用 sudo ，不要使用rootsu - cephusermkdir my-clustercd my-cluster# 创建集群，用 new 命令，并指定几个主机安装初始ceph Monitor 服务# 在centos 7-ceph-1上操作ceph-deploy new centos 7-ceph-1# 此时会在当前目录生成 ceph 配置文件、日志和密钥文件# 注意实际生效的配置文件将在 /etc/ceph/ceph.conf （当前还未生生成），最佳实践为修改 $HOME 目录下的文件配置，通过 deploy 工具分发到各节点lsceph.conf ceph-deploy-ceph.log ceph.mon.keyring# 需要修改一下配置文件echo \"public network = 192.168.153.0/24\" &gt;&gt; ceph.confecho \"cluster network = 192.168.153.0/24\" &gt;&gt; ceph.confecho \"osd pool default size = 3\" &gt;&gt; ceph.confecho \"osd pool default min size = 2\" &gt;&gt; ceph.confecho \"mon_max_pg_per_osd = 650\" &gt;&gt; ceph.confecho \"[osd]\" &gt;&gt; ceph.confecho \"mon_osd_backfillfull_ratio=0.7\" &gt;&gt; ceph.confecho \"mon_osd_full_ratio=0.8\" &gt;&gt; ceph.confecho \"mon_osd_nearfull_ratio=0.6\" &gt;&gt; ceph.confecho \"osd_failsafe_full_ratio=0.8\" &gt;&gt; ceph.conf# 部署monitor和生成keysceph-deploy mon create-initialls -l *.keyring# 复制文件到node节点ceph-deploy --overwrite-conf admin centos 7-ceph-1 centos 7-ceph-2 centos 7-ceph-3# 部署manager （luminous+）12及以后的版本需要部署ceph-deploy mgr create centos 7-ceph-1 创建 OSD（对象存储后台进程） 1234567891011#在centos 7-ceph-1用cephuser用户执行ceph-deploy osd create --data /dev/sdb centos 7-ceph-1 ceph-deploy osd create --data /dev/sdb centos 7-ceph-2 ceph-deploy osd create --data /dev/sdb centos 7-ceph-3 # 查看 osd tree和ceph集群状态sudo ceph osd treesudo ceph -ssudo ceph health detail //HEALTH_OK# 至此 基本ceph安装完毕 高可用配置(mon、mgr) 123456789# 添加monitorceph-deploy mon add centos 7-ceph-2ceph-deploy mon add centos 7-ceph-3# 添加managerceph-deploy mgr create centos 7-ceph-2 centos 7-ceph-3# 卸载monitorceph-deploy mon destroy centos 7-ceph-2 部署RGWCeph部署RGW以兼容S3/Swift接口，即可以使用S3或Swift的命令行工具或SDK来使用ceph。 部署RGW网关 12345678910111213141516171819202122232425# 启动 RGW，注意开放的网关端口，默认7480ceph-deploy rgw create centos 7-ceph-1 // The Ceph Object Gateway (RGW) is now running on host centos 7-ceph-1 and default port 7480# 访问测试curl -I http://centos 7-ceph-1:7480/HTTP/1.1 200 OKx-amz-request-id: tx000000000000000000001-005d0b0e2a-1018-defaultContent-Type: application/xmlContent-Length: 0Date: Thu, 20 Jun 2019 04:40:11 GMT# 页面访问测试 # 至此 Ceph 端对象访问接口已经部署完成，可使用s3 工具访问 # 附其他操作 # 附1：更改开放端口，需重启RGW# 修改配置 /etc/ceph/ceph.conf，使用 rgw 监听在 80 端口 [client.rgw.lab1]rgw_frontends = \"civetweb port=80\" # 附2：重启 RGWsystemctl restart ceph-radosgw@rgw.k8s113 对象存储服务与测试（S3/Swift）离线安装 s3cmd 12345rpm -ivh python-dateutil-1.5-7.el7.noarch.rpm python-magic-5.11-35.el7.noarch.rpm s3cmd-2.0.2-1.el7.noarch.rpms3cmd --versions3cmd version 2.0.2 对象存储测试（使用s3cmd测试） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141# 测试# 创建S3所需要的poolsudo ceph osd pool create .rgw 128 128sudo ceph osd pool create .rgw.root 128 128sudo ceph osd pool create .rgw.control 128 128sudo ceph osd pool create .rgw.gc 128 128sudo ceph osd pool create .rgw.buckets 128 128sudo ceph osd pool create .rgw.buckets.index 128 128sudo ceph osd pool create .rgw.buckets.extra 128 128sudo ceph osd pool create .log 128 128sudo ceph osd pool create .intent-log 128 128sudo ceph osd pool create .usage 128 128sudo ceph osd pool create .users 128 128sudo ceph osd pool create .users.email 128 128sudo ceph osd pool create .users.swift 128 128sudo ceph osd pool create .users.uid 128 128# 查看rados lspools# 访问测试curl -I http://192.168.153.51:7480/# 创建S3用户# 注意：保存命令返回的 user access_key secret_keysudo radosgw-admin user create --uid=s3_user --display-name=s3_user --email=s3@s3.com\"access_key\": \"FZ95GV7T0WU8OVOL4YH9\",\"secret_key\": \"WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPK\"# 创建admin用户radosgw-admin user create --uid=admin --display-name=admin\"access_key\": \"B3M00AHVE9FEXCQ8D0L9\",\"secret_key\": \"JVJG1O4LDGam35EBghXGsgrdOwhqGzVKPtR6ntw4\"# 允许admin读写所有users信息radosgw-admin caps add --uid=admin --caps=\"users=*\"# 允许admin读写所有的usage信息radosgw-admin caps add --uid=admin --caps=\"usage=read,write\"# 配置s3cmd（当前在 ceph目录下，附完整过程，输入之前创建的 s3_user_tydic 用户密钥） s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key: FZ95GV7T0WU8OVOL4YH9Secret Key: WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPKDefault Region [US]:Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.S3 Endpoint [s3.amazonaws.com]: 192.168.153.51:7480 //注意：此处必须使用 ip:port 形式，使用域名方式将会导致从s3cmd创建的桶必须是以大写字母开头，此命名与亚马逊官方桶命名要求相悖，建议修改为ip:port 方式。Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 192.168.153.51:7480 //ip:port 形式Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: //空Path to GPG program [/bin/gpg]: //空When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [Yes]: noOn some networks all internet access must go through a HTTP proxy.Try setting it here if you can't connect to S3 directlyHTTP Proxy server name: //空Test access with supplied credentials? [Y/n] yPlease wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] yConfiguration saved to '/home/ceph/.s3cfg'# 注意 s3 配置文件生成位置修该生成的配置文件，后续可做自定义修改# 重点配置项示例，其他项均为默认项[default]access_key = FZ95GV7T0WU8OVOL4YH9secret_key = WFgeHmpjywZ3AAWDjc3atNOsGAP7gJkNmUVWHoPKhost_base = 192.168.153.51:7480host_bucket = 192.168.153.51:7480use_https = False# 创建Bucket（注意命令行端bucket首字母必须大写）s3cmd mb s3://MyBucket_1s3cmd ls# 上传Objectecho 'hello ceph block storage s3' &gt; hello.txts3cmd put hello.txt s3://MyBucket_1# 查看Objects3cmd ls s3://MyBucket_1# 下载Objectcd /tmps3cmd get s3://mybucket/hello.txtcat hello.txt# 删除bucket下所有对象s3cmd del -rf s3://MyBucket_1 s3cmd ls -r s3://MyBucket_1# 删除Buckets3cmd mb s3://MyBucket_1s3cmd rb s3://MyBucket_1# 其他操作# 删除S3用户radosgw-admin user rm --uid=s3_user_tydicradosgw-admin user rm --uid=admin#调整pool副本数ceph osd pool set default.rgw.buckets.data size 3ceph osd pool set default.rgw.buckets.data min_size 2# 删除poolceph osd pool delete .rgw .rgw --yes-i-really-really-mean-itceph osd pool delete .rgw.root .rgw.root --yes-i-really-really-mean-itceph osd pool delete .rgw.control .rgw.control --yes-i-really-really-mean-itceph osd pool delete .rgw.gc .rgw.gc --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets .rgw.buckets --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.index .rgw.buckets.index --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.extra .rgw.buckets.extra --yes-i-really-really-mean-itceph osd pool delete .log .log --yes-i-really-really-mean-itceph osd pool delete .intent-log .intent-log --yes-i-really-really-mean-itceph osd pool delete .usage .usage --yes-i-really-really-mean-itceph osd pool delete .users .users --yes-i-really-really-mean-itceph osd pool delete .users.email .users.email --yes-i-really-really-mean-itceph osd pool delete .users.swift .users.swift --yes-i-really-really-mean-itceph osd pool delete .users.uid .users.uid --yes-i-really-really-mean-it Ceph DashboardCeph Dashboard介绍Ceph 的监控可视化界面方案很多—-grafana、Kraken。但是从Luminous开始，Ceph 提供了原生的Dashboard功能，通过Dashboard可以获取Ceph集群的各种基本状态信息。 配置Ceph Dashboard安装和配置12345678910111213141516171819202122# 1、在每个mgr节点安装 yum install ceph-mgr-dashboard # 2、开启mgr功能 ceph mgr module enable dashboard # 3、生成并安装自签名的证书 ceph dashboard create-self-signed-cert # 4、创建一个dashboard登录用户名密码 ceph dashboard ac-user-create super 123456 administrator # 5、查看服务访问方式 ceph mgr services&#123; \"dashboard\": \"https://centos 7-ceph-1:8443/\"&#125;#安装完成#可选#修改ipsudo ceph config set mgr mgr/dashboard/server_addr 192.168.153.51#修改端口sudo ceph config set mgr mgr/dashboard/server_port 8443#禁用httpssudo ceph config set mgr mgr/dashboard/ssl false 安装问题汇总1.格式化磁盘时提示忙 1mkfs.xfs: cannot open /dev/sdb: Device or resource busy 解决方法 123456789lsblk #显示部分磁盘正常，部分下面有-ceph-**等标识，用ilo多次格式化磁盘作raid0均无效果dmsetup ls #查看谁在占用，找到ceph-**字样（ceph-**为lsblk显示的块设备具体信息）#使用dmsetup 删除字样dmsetup remove ceph-**lsblk #查看设备信息，可以看到ceph-**等标识等标识消失ceph-deploy disk zap centos7-ceph-3 /dev/sdb #格式化磁盘sudo parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%sudo mkfs.xfs /dev/sdb -f 2.ceph-deploy new centos 7-ceph-1 时，提示 ImportError: No module named pkg_resources 1234567[cephuser@centos 7-ceph-1 my-cluster]$ ceph-deploy new centos 7-ceph-1Traceback (most recent call last): File \"/bin/ceph-deploy\", line 18, in &lt;module&gt; from ceph_deploy.cli import main File \"/usr/lib/python2.7/site-packages/ceph_deploy/cli.py\", line 1, in &lt;module&gt; import pkg_resourcesImportError: No module named pkg_resources 解决方法 12345#安装pipsudo yum -y install epel-releasesudo yum -y install python-pip#安装distribute类库pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com distrbute 3.如果安装出错如何清理 12345# 卸载Ceph安装包ceph-deploy purge &lt;hostname&gt;# 清理配置ceph-deploy purgedata &lt;hostname&gt;ceph-deploy forgetkeys 安装ceph dashboard 提示 No module 1Module 'dashboard' has failed dependency: No module named urllib3.exceptions 解决方法 12#pip安装 urllib3pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com urllib3","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zhangjunjunah.github.io/tags/Linux/"},{"name":"Ceph","slug":"Ceph","permalink":"https://zhangjunjunah.github.io/tags/Ceph/"}]}]}